{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "We utilize [TORCHMETRICS](https://lightning.ai/docs/torchmetrics/stable/references/metric.html) to implement our train loss, validation and test metrics. The class can help us \n",
    "\n",
    "1. Handles the transfer of metric states to correct device \n",
    "2. Handles the synchronization of metric states across processes \n",
    "\n",
    "The three core methods of the base class are `add_state()`, `forward()`, and `reset()` which should never be overwritten by child classes. Instead, we should overwrite `update()` and `compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Metric, MeanSquaredError\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumExceptBatchKL(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_value', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, p, q) -> None:\n",
    "        self.total_value += F.kl_div(q, p, reduction='sum')\n",
    "        self.total_samples += p.size(0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_value / self.total_samples\n",
    "    \n",
    "class SumExceptBatchMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_value', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, values) -> None:\n",
    "        self.total_value += torch.sum(values)\n",
    "        self.total_samples += values.shape[0]\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_value / self.total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_ce', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor) -> None:\n",
    "        \"\"\" Update state with predictions and targets.\n",
    "            preds: Predictions from model   (bs * n, d) or (bs * n * n, d)\n",
    "            target: Ground truth values     (bs * n, d) or (bs * n * n, d). \"\"\"\n",
    "        target = torch.argmax(target, dim=-1)\n",
    "        output = F.cross_entropy(preds, target, reduction='sum')\n",
    "        self.total_ce += output\n",
    "        self.total_samples += preds.size(0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_ce / self.total_samples\n",
    "\n",
    "class NLL(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_nll', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, batch_nll) -> None:\n",
    "        self.total_nll += torch.sum(batch_nll)\n",
    "        self.total_samples += batch_nll.numel()\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_nll / self.total_samples\n",
    "\n",
    "\n",
    "class TrainLossDiscrete(nn.Module):\n",
    "    \"\"\" Train with Cross entropy\"\"\"\n",
    "    def __init__(self, lambda_train):\n",
    "        super().__init__()\n",
    "        self.node_loss = CrossEntropyMetric()\n",
    "        self.edge_loss = CrossEntropyMetric()\n",
    "        self.y_loss = CrossEntropyMetric()\n",
    "        self.lambda_train = lambda_train\n",
    "\n",
    "    def forward(self, masked_pred_X, masked_pred_E, pred_y, true_X, true_E, true_y, log: bool):\n",
    "        \"\"\" Compute train metrics\n",
    "        masked_pred_X : tensor -- (bs, n, dx)\n",
    "        masked_pred_E : tensor -- (bs, n, n, de)\n",
    "        pred_y : tensor -- (bs, )\n",
    "        true_X : tensor -- (bs, n, dx)\n",
    "        true_E : tensor -- (bs, n, n, de)\n",
    "        true_y : tensor -- (bs, )\n",
    "        log : boolean. \"\"\"\n",
    "        true_X = torch.reshape(true_X, (-1, true_X.size(-1)))  # (bs * n, dx)\n",
    "        true_E = torch.reshape(true_E, (-1, true_E.size(-1)))  # (bs * n * n, de)\n",
    "        masked_pred_X = torch.reshape(masked_pred_X, (-1, masked_pred_X.size(-1)))  # (bs * n, dx)\n",
    "        masked_pred_E = torch.reshape(masked_pred_E, (-1, masked_pred_E.size(-1)))   # (bs * n * n, de)\n",
    "\n",
    "        # Remove masked rows\n",
    "        mask_X = (true_X != 0.).any(dim=-1)\n",
    "        mask_E = (true_E != 0.).any(dim=-1)\n",
    "\n",
    "        flat_true_X = true_X[mask_X, :]\n",
    "        flat_pred_X = masked_pred_X[mask_X, :]\n",
    "\n",
    "        flat_true_E = true_E[mask_E, :]\n",
    "        flat_pred_E = masked_pred_E[mask_E, :]\n",
    "\n",
    "        loss_X = self.node_loss(flat_pred_X, flat_true_X) if true_X.numel() > 0 else 0.0\n",
    "        loss_E = self.edge_loss(flat_pred_E, flat_true_E) if true_E.numel() > 0 else 0.0\n",
    "        loss_y = self.y_loss(pred_y, true_y) if true_y.numel() > 0 else 0.0\n",
    "        \n",
    "        return loss_X + self.lambda_train[0] * loss_E + self.lambda_train[1] * loss_y\n",
    "\n",
    "    def reset(self):\n",
    "        for metric in [self.node_loss, self.edge_loss, self.y_loss]:\n",
    "            metric.reset()\n",
    "\n",
    "    def log_epoch_metrics(self):\n",
    "        epoch_node_loss = self.node_loss.compute() if self.node_loss.total_samples > 0 else -1\n",
    "        epoch_edge_loss = self.edge_loss.compute() if self.edge_loss.total_samples > 0 else -1\n",
    "        epoch_y_loss = self.train_y_loss.compute() if self.y_loss.total_samples > 0 else -1\n",
    "\n",
    "        to_log = {\"train_epoch/x_CE\": epoch_node_loss,\n",
    "                  \"train_epoch/E_CE\": epoch_edge_loss,\n",
    "                  \"train_epoch/y_CE\": epoch_y_loss}\n",
    "\n",
    "        return to_log\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
