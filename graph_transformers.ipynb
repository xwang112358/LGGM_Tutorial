{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Transformer Model\n",
    "\n",
    "### XEyTransformerLayer\n",
    "```\n",
    "def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,\n",
    "                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,\n",
    "                 layer_norm_eps: float = 1e-5)\n",
    "```\n",
    "Transformer that updates node, edge and global features\n",
    "- $\\mathbf{X}$ : Node features matrix, shape ( bs, $n, d_x$ )\n",
    "- $\\mathbf{E}$: Edge features matrix, shape (bs, $n, n, d_e$ )\n",
    "- $\\mathbf{y}$ : Global features vector, shape (bs, $d_y$ )\n",
    "- **node_mask**: Node mask, shape (bs, $n$ )\n",
    "- $\\mathbf{W}$ : Weight matrices for the linear layers\n",
    "- $\\mathbf{b}$ : Bias vectors for the linear layers\n",
    "- **LN**: Layer normalization\n",
    "- **Dropout**: Dropout operation\n",
    "- **ReLU**: Rectified Linear Unit activation function\n",
    "\n",
    "#### Self Attention\n",
    "\\begin{equation*}\n",
    "\\mathbf{X}_{\\text {new }}, \\mathbf{E}_{\\text {new }}, \\mathbf{y}_{\\text {new }}=\\operatorname{SelfAttn}\\left(\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text { node } _ \\text {mask }\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "#### Residual and Layer Normalization \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{gathered}\n",
    "\\mathbf{X}_{\\text {residual }}=\\mathbf{X}+\\operatorname{Dropout}\\left(\\mathbf{X}_{\\text {new }}\\right) \\\\\n",
    "\\mathbf{X}=\\operatorname{LN}\\left(\\mathbf{X}_{\\text {residual }}\\right)\n",
    "\\end{gathered}\n",
    "\\end{equation*}\n",
    "\n",
    "####  Feed-Forward Layer \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "\\mathbf{X}_{\\mathrm{ff}}=\\operatorname{Dropout}\\left(\\operatorname{ReLU}\\left(\\mathbf{X} \\mathbf{W}_{X 1}+\\mathbf{b}_{X 1}\\right)\\right) \\\\\n",
    "\\mathbf{X}_{\\mathrm{ff}}=\\mathbf{X}_{\\mathrm{ff}} \\mathbf{W}_{X 2}+\\mathbf{b}_{X 2} \\\\\n",
    "\\mathbf{X}_{\\mathrm{ff}}=\\operatorname{Dropout}\\left(\\mathbf{X}_{\\mathrm{ff}}\\right) \\\\\n",
    "\\mathbf{X}=\\mathrm{LN}\\left(\\mathbf{X}+\\mathbf{X}_{\\mathrm{ff}}\\right)\n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "### NodeEdgeBlock\n",
    "Self-attention layer that also updates the representations on the edges\n",
    "\n",
    "```\n",
    "def __init__(self, dx, de, dy, n_head, **kwargs):\n",
    "```\n",
    "\n",
    "#### Linear Projections and Maskings \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\mathbf{Q}=\\mathbf{X} \\mathbf{W}_Q \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{K}=\\mathbf{X} \\mathbf{W}_K \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{V}=\\mathbf{X} \\mathbf{W}_V \\odot \\mathbf{x}_{\\text {mask }}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "#### Reshape for Multi-head Attention\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\mathbf{Q} = \\operatorname{reshape}\\left(\\mathbf{Q}, \\left(\\mathrm{bs}, n, \\mathrm{n_head}, \\mathrm{df}\\right)\\right) \\\\\n",
    "&\\mathbf{K} = \\operatorname{reshape}\\left(\\mathbf{K}, \\left(\\mathrm{bs}, n, \\mathrm{n_head}, \\mathrm{df}\\right)\\right) \\\\\n",
    "&\\mathbf{V} = \\operatorname{reshape}\\left(\\mathbf{V}, \\left(\\mathrm{bs}, n, \\mathrm{n_head}, \\mathrm{df}\\right)\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "#### Compute Unnormalized Attention Scores\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\frac{(\\mathbf{Q} \\odot \\mathbf{x}_{\\text{mask}}) \\cdot (\\mathbf{K} \\odot \\mathbf{x}_{\\text{mask}})^{\\top}}{\\sqrt{\\mathrm{df}}}\n",
    "$$\n",
    "\n",
    "#### Incorporate Edge Features \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{gathered}\n",
    "\\mathbf{E}_{\\text {mul }}=\\operatorname{reshape}\\left(\\mathbf{E} \\mathbf{W}_{\\text {e_mul }},\\left(\\mathrm{bs}, n, n, \\mathrm{n} _ \\text {head, df }\\right)\\right) \\odot \\mathbf{e}_{\\text {mask1 }} \\odot \\mathbf{e}_{\\text {mask2 }} \\\\\n",
    "\\mathbf{E}_{\\text {add }}=\\operatorname{reshape}\\left(\\mathbf{E} \\mathbf{W}_{\\text {e_add }},\\left(\\mathrm{bs}, n, n, \\mathrm{n} _ \\text {head, df }\\right)\\right) \\odot \\mathbf{e}_{\\text {mask1 }} \\odot \\mathbf{e}_{\\text {mask2 }} \\\\\n",
    "\\mathbf{Y}=\\mathbf{Y} \\odot\\left(\\mathbf{E}_{\\text {mul }}+1\\right)+\\mathbf{E}_{\\text {add }}\n",
    "\\end{gathered}\n",
    "\\end{equation}\n",
    "\n",
    "#### Compute Weighted Values\n",
    "\n",
    "\n",
    "   \\[\n",
    "   \\mathbf{V} = \\operatorname{reshape}(\\mathbf{V}, (\\text{bs}, 1, n, \\text{n_head}, \\text{df})) \\odot \\mathbf{x}_{\\text{mask}} \\quad (\\text{expand mask})\n",
    "   \\]\n",
    "   \\[\n",
    "   \\mathbf{weighted_V} = \\sum_{j} \\mathbf{A}_{ij} \\mathbf{V}_{j}\n",
    "   \\]\n",
    "   \\[\n",
    "   \\mathbf{weighted_V} = \\operatorname{reshape}(\\mathbf{weighted_V}, (\\text{bs}, n, d_x))\n",
    "   \\]\n",
    "\n",
    "#### Incorporate Global Features to Node Features\n",
    "   \\[\n",
    "   \\mathbf{y}_{\\text{x_add}} = \\mathbf{y} \\mathbf{W}_{\\text{y_x_add}} \\odot \\mathbf{x}_{\\text{mask}}\n",
    "   \\]\n",
    "   \\[\n",
    "   \\mathbf{y}_{\\text{x_mul}} = \\mathbf{y} \\mathbf{W}_{\\text{y_x_mul}} \\odot \\mathbf{x}_{\\text{mask}}\n",
    "   \\]\n",
    "   \\[\n",
    "   \\mathbf{newX} = \\mathbf{y}_{\\text{x_add}} + (\\mathbf{y}_{\\text{x_mul}} + 1) \\odot \\mathbf{weighted_V}\n",
    "   \\]\n",
    "\n",
    "#### Output Node Features\n",
    "   \\[\n",
    "   \\mathbf{newX} = \\mathbf{newX} \\mathbf{W}_{\\text{x_out}} \\odot \\mathbf{x}_{\\text{mask}}\n",
    "   \\]\n",
    "\n",
    "### Summary\n",
    "\n",
    "The update of the node features \\( \\mathbf{X} \\) in the `NodeEdgeBlock` involves:\n",
    "1. Projecting \\( \\mathbf{X} \\) to queries (\\(\\mathbf{Q}\\)), keys (\\(\\mathbf{K}\\)), and values (\\(\\mathbf{V}\\)).\n",
    "2. Computing scaled dot-product attention scores.\n",
    "3. Incorporating edge features into the attention scores.\n",
    "4. Applying a masked softmax to obtain attention weights.\n",
    "5. Computing weighted sums of values based on the attention weights.\n",
    "6. Incorporating global features into the updated node features.\n",
    "7. Producing the final updated node features.\n",
    "\n",
    "### GraphTransformer \n",
    "\n",
    "The `GraphTransformer` class combines multi-layer perceptrons (MLPs) and transformer layers to process graph data. \n",
    "\n",
    "\n",
    "1. **Initial Feature Processing:**\n",
    "   \\[\n",
    "   \\begin{aligned}\n",
    "   &\\mathbf{X}_{\\text{in}} = \\sigma(\\mathbf{W}_{1X} \\sigma(\\mathbf{W}_{0X} \\mathbf{X} + \\mathbf{b}_{0X}) + \\mathbf{b}_{1X}) \\\\\n",
    "   &\\mathbf{E}_{\\text{in}} = \\frac{1}{2} (\\sigma(\\mathbf{W}_{1E} \\sigma(\\mathbf{W}_{0E} \\mathbf{E} + \\mathbf{b}_{0E}) + \\mathbf{b}_{1E}) + \\sigma(\\mathbf{W}_{1E} \\sigma(\\mathbf{W}_{0E} \\mathbf{E}^T + \\mathbf{b}_{0E}) + \\mathbf{b}_{1E})) \\\\\n",
    "   &\\mathbf{y}_{\\text{in}} = \\sigma(\\mathbf{W}_{1y} \\sigma(\\mathbf{W}_{0y} \\mathbf{y} + \\mathbf{b}_{0y}) + \\mathbf{b}_{1y}) \\\\\n",
    "   \\end{aligned}\n",
    "   \\]\n",
    "\n",
    "2. **Transformer Layers:**\n",
    "   For each transformer layer \\(i\\):\n",
    "   \\[\n",
    "   \\mathbf{X}, \\mathbf{E}, \\mathbf{y} = \\text{tf_layers}[i](\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text{node_mask})\n",
    "   \\]\n",
    "\n",
    "3. **Final Feature Processing:**\n",
    "   \\[\n",
    "   \\begin{aligned}\n",
    "   &\\mathbf{X}_{\\text{out}} = \\sigma(\\mathbf{W}_{3X} \\sigma(\\mathbf{W}_{2X} \\mathbf{X} + \\mathbf{b}_{2X}) + \\mathbf{b}_{3X}) + \\mathbf{X} \\\\\n",
    "   &\\mathbf{E}_{\\text{out}} = \\sigma(\\mathbf{W}_{3E} \\sigma(\\mathbf{W}_{2E} \\mathbf{E} + \\mathbf{b}_{2E}) + \\mathbf{b}_{3E}) \\cdot \\text{diag_mask} + \\mathbf{E} \\cdot \\text{diag_mask} \\\\\n",
    "   &\\mathbf{y}_{\\text{out}} = \\sigma(\\mathbf{W}_{3y} \\sigma(\\mathbf{W}_{2y} \\mathbf{y} + \\mathbf{b}_{2y}) + \\mathbf{b}_{3y}) + \\mathbf{y} \\\\\n",
    "   \\end{aligned}\n",
    "   \\]\n",
    "\n",
    "4. **Symmetrizing Edge Features:**\n",
    "   \\[\n",
    "   \\mathbf{E}_{\\text{out}} = \\frac{1}{2} (\\mathbf{E}_{\\text{out}} + \\mathbf{E}_{\\text{out}}^T)\n",
    "   \\]\n",
    "\n",
    "5. **Return Masked Outputs:**\n",
    "   \\[\n",
    "   \\text{utils.PlaceHolder}(\\mathbf{X} = \\mathbf{X}_{\\text{out}}, \\mathbf{E} = \\mathbf{E}_{\\text{out}}, \\mathbf{y} = \\mathbf{y}_{\\text{out}}).mask(\\text{node_mask})\n",
    "   \\]\n",
    "\n",
    "### Summary\n",
    "The `GraphTransformer` processes node (\\(\\mathbf{X}\\)), edge (\\(\\mathbf{E}\\)), and global (\\(\\mathbf{y}\\)) features through MLPs, applies multiple transformer layers to update the features, and then processes the updated features through output MLPs before returning the final results with symmetric edge features and masked nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Transformer $\\phi_{\\theta}$: the Denoising Model\n",
    "class Xtoy(nn.Module):\n",
    "    def __init__(self, dx, dy):\n",
    "        \"\"\" Map node features to global features \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * dx, dy)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" X: bs, n, dx. \"\"\"\n",
    "        m = X.mean(dim=1)\n",
    "        mi = X.min(dim=1)[0]\n",
    "        ma = X.max(dim=1)[0]\n",
    "        std = X.std(dim=1)\n",
    "        z = torch.hstack((m, mi, ma, std))\n",
    "        out = self.lin(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Etoy(nn.Module):\n",
    "    def __init__(self, d, dy):\n",
    "        \"\"\" Map edge features to global features. \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * d, dy)\n",
    "\n",
    "    def forward(self, E):\n",
    "        \"\"\" E: bs, n, n, de\n",
    "            Features relative to the diagonal of E could potentially be added.\n",
    "        \"\"\"\n",
    "        m = E.mean(dim=(1, 2))\n",
    "        mi = E.min(dim=2)[0].min(dim=1)[0]\n",
    "        ma = E.max(dim=2)[0].max(dim=1)[0]\n",
    "        std = torch.std(E, dim=(1, 2))\n",
    "        z = torch.hstack((m, mi, ma, std))\n",
    "        out = self.lin(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "def masked_softmax(x, mask, **kwargs):\n",
    "    if mask.sum() == 0:\n",
    "        return x\n",
    "    x_masked = x.clone()\n",
    "    x_masked[mask == 0] = -float(\"inf\")\n",
    "    return torch.softmax(x_masked, **kwargs)\n",
    "class XEyTransformerLayer(nn.Module):\n",
    "    \"\"\" Transformer that updates node, edge and global features\n",
    "        d_x: node features\n",
    "        d_e: edge features\n",
    "        dz : global features\n",
    "        n_head: the number of heads in the multi_head_attention\n",
    "        dim_feedforward: the dimension of the feedforward network model after self-attention\n",
    "        dropout: dropout probablility. 0 to disable\n",
    "        layer_norm_eps: eps value in layer normalizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,\n",
    "                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,\n",
    "                 layer_norm_eps: float = 1e-5, device=None, dtype=None) -> None:\n",
    "        kw = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = NodeEdgeBlock(dx, de, dy, n_head, **kw)\n",
    "\n",
    "        self.linX1 = Linear(dx, dim_ffX, **kw)\n",
    "        self.linX2 = Linear(dim_ffX, dx, **kw)\n",
    "        self.normX1 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.normX2 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutX1 = Dropout(dropout)\n",
    "        self.dropoutX2 = Dropout(dropout)\n",
    "        self.dropoutX3 = Dropout(dropout)\n",
    "\n",
    "        self.linE1 = Linear(de, dim_ffE, **kw)\n",
    "        self.linE2 = Linear(dim_ffE, de, **kw)\n",
    "        self.normE1 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.normE2 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutE1 = Dropout(dropout)\n",
    "        self.dropoutE2 = Dropout(dropout)\n",
    "        self.dropoutE3 = Dropout(dropout)\n",
    "\n",
    "        self.lin_y1 = Linear(dy, dim_ffy, **kw)\n",
    "        self.lin_y2 = Linear(dim_ffy, dy, **kw)\n",
    "        self.norm_y1 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.norm_y2 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.dropout_y1 = Dropout(dropout)\n",
    "        self.dropout_y2 = Dropout(dropout)\n",
    "        self.dropout_y3 = Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, X: Tensor, E: Tensor, y, node_mask: Tensor):\n",
    "        \"\"\" Pass the input through the encoder layer.\n",
    "            X: (bs, n, d)\n",
    "            E: (bs, n, n, d)\n",
    "            y: (bs, dy)\n",
    "            node_mask: (bs, n) Mask for the src keys per batch (optional)\n",
    "            Output: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "\n",
    "        newX, newE, new_y = self.self_attn(X, E, y, node_mask=node_mask)\n",
    "\n",
    "        newX_d = self.dropoutX1(newX)\n",
    "        X = self.normX1(X + newX_d)\n",
    "\n",
    "        newE_d = self.dropoutE1(newE)\n",
    "        E = self.normE1(E + newE_d)\n",
    "\n",
    "        new_y_d = self.dropout_y1(new_y)\n",
    "        y = self.norm_y1(y + new_y_d)\n",
    "\n",
    "        ff_outputX = self.linX2(self.dropoutX2(self.activation(self.linX1(X))))\n",
    "        ff_outputX = self.dropoutX3(ff_outputX)\n",
    "        X = self.normX2(X + ff_outputX)\n",
    "\n",
    "        ff_outputE = self.linE2(self.dropoutE2(self.activation(self.linE1(E))))\n",
    "        ff_outputE = self.dropoutE3(ff_outputE)\n",
    "        E = self.normE2(E + ff_outputE)\n",
    "\n",
    "        ff_output_y = self.lin_y2(self.dropout_y2(self.activation(self.lin_y1(y))))\n",
    "        ff_output_y = self.dropout_y3(ff_output_y)\n",
    "        y = self.norm_y2(y + ff_output_y)\n",
    "\n",
    "        return X, E, y\n",
    "\n",
    "class NodeEdgeBlock(nn.Module):\n",
    "    \"\"\" Self attention layer that also updates the representations on the edges. \"\"\"\n",
    "    def __init__(self, dx, de, dy, n_head, **kwargs):\n",
    "        super().__init__()\n",
    "        assert dx % n_head == 0, f\"dx: {dx} -- nhead: {n_head}\"\n",
    "        self.dx = dx\n",
    "        self.de = de\n",
    "        self.dy = dy\n",
    "        self.df = int(dx / n_head)\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # Attention\n",
    "        self.q = Linear(dx, dx)\n",
    "        self.k = Linear(dx, dx)\n",
    "        self.v = Linear(dx, dx)\n",
    "\n",
    "        # FiLM E to X\n",
    "        self.e_add = Linear(de, dx)\n",
    "        self.e_mul = Linear(de, dx)\n",
    "\n",
    "        # FiLM y to E\n",
    "        self.y_e_mul = Linear(dy, dx)           # Warning: here it's dx and not de\n",
    "        self.y_e_add = Linear(dy, dx)\n",
    "\n",
    "        # FiLM y to X\n",
    "        self.y_x_mul = Linear(dy, dx)\n",
    "        self.y_x_add = Linear(dy, dx)\n",
    "\n",
    "        # Process y\n",
    "        self.y_y = Linear(dy, dy)\n",
    "        self.x_y = Xtoy(dx, dy)\n",
    "        self.e_y = Etoy(de, dy)\n",
    "\n",
    "        # Output layers\n",
    "        self.x_out = Linear(dx, dx)\n",
    "        self.e_out = Linear(dx, de)\n",
    "        self.y_out = nn.Sequential(nn.Linear(dy, dy), nn.ReLU(), nn.Linear(dy, dy))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        \"\"\"\n",
    "        :param X: bs, n, d        node features\n",
    "        :param E: bs, n, n, d     edge features\n",
    "        :param y: bs, dz           global features\n",
    "        :param node_mask: bs, n\n",
    "        :return: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "        bs, n, _ = X.shape\n",
    "        x_mask = node_mask.unsqueeze(-1)        # bs, n, 1\n",
    "        e_mask1 = x_mask.unsqueeze(2)           # bs, n, 1, 1\n",
    "        e_mask2 = x_mask.unsqueeze(1)           # bs, 1, n, 1\n",
    "\n",
    "        # 1. Map X to keys and queries\n",
    "        Q = self.q(X) * x_mask           # (bs, n, dx)\n",
    "        K = self.k(X) * x_mask           # (bs, n, dx)\n",
    "        diffusion_utils.assert_correctly_masked(Q, x_mask)\n",
    "        # 2. Reshape to (bs, n, n_head, df) with dx = n_head * df\n",
    "\n",
    "        Q = Q.reshape((Q.size(0), Q.size(1), self.n_head, self.df))\n",
    "        K = K.reshape((K.size(0), K.size(1), self.n_head, self.df))\n",
    "\n",
    "        Q = Q.unsqueeze(2)                              # (bs, 1, n, n_head, df)\n",
    "        K = K.unsqueeze(1)                              # (bs, n, 1, n head, df)\n",
    "\n",
    "        # Compute unnormalized attentions. Y is (bs, n, n, n_head, df)\n",
    "        Y = Q * K\n",
    "        Y = Y / math.sqrt(Y.size(-1))\n",
    "        diffusion_utils.assert_correctly_masked(Y, (e_mask1 * e_mask2).unsqueeze(-1))\n",
    "\n",
    "        E1 = self.e_mul(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E1 = E1.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))\n",
    "\n",
    "        E2 = self.e_add(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E2 = E2.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))\n",
    "\n",
    "        # Incorporate edge features to the self attention scores.\n",
    "        Y = Y * (E1 + 1) + E2                  # (bs, n, n, n_head, df)\n",
    "\n",
    "        # Incorporate y to E\n",
    "        newE = Y.flatten(start_dim=3)                      # bs, n, n, dx\n",
    "        ye1 = self.y_e_add(y).unsqueeze(1).unsqueeze(1)  # bs, 1, 1, de\n",
    "        ye2 = self.y_e_mul(y).unsqueeze(1).unsqueeze(1)\n",
    "        newE = ye1 + (ye2 + 1) * newE\n",
    "\n",
    "        # Output E\n",
    "        newE = self.e_out(newE) * e_mask1 * e_mask2      # bs, n, n, de\n",
    "        diffusion_utils.assert_correctly_masked(newE, e_mask1 * e_mask2)\n",
    "\n",
    "        # Compute attentions. attn is still (bs, n, n, n_head, df)\n",
    "        softmax_mask = e_mask2.expand(-1, n, -1, self.n_head)    # bs, 1, n, 1\n",
    "        attn = masked_softmax(Y, softmax_mask, dim=2)  # bs, n, n, n_head\n",
    "\n",
    "        V = self.v(X) * x_mask                        # bs, n, dx\n",
    "        V = V.reshape((V.size(0), V.size(1), self.n_head, self.df))\n",
    "        V = V.unsqueeze(1)                                     # (bs, 1, n, n_head, df)\n",
    "\n",
    "        # Compute weighted values\n",
    "        weighted_V = attn * V\n",
    "        weighted_V = weighted_V.sum(dim=2)\n",
    "\n",
    "        # Send output to input dim\n",
    "        weighted_V = weighted_V.flatten(start_dim=2)            # bs, n, dx\n",
    "\n",
    "        # Incorporate y to X\n",
    "        yx1 = self.y_x_add(y).unsqueeze(1)\n",
    "        yx2 = self.y_x_mul(y).unsqueeze(1)\n",
    "        newX = yx1 + (yx2 + 1) * weighted_V\n",
    "\n",
    "        # Output X\n",
    "        newX = self.x_out(newX) * x_mask\n",
    "        diffusion_utils.assert_correctly_masked(newX, x_mask)\n",
    "\n",
    "        # Process y based on X axnd E\n",
    "        y = self.y_y(y)\n",
    "        e_y = self.e_y(E)\n",
    "        x_y = self.x_y(X)\n",
    "        new_y = y + x_y + e_y\n",
    "        new_y = self.y_out(new_y)               # bs, dy\n",
    "\n",
    "        return newX, newE, new_y\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    n_layers : int -- number of layers\n",
    "    dims : dict -- contains dimensions for each feature type\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers: int, input_dims: dict, hidden_mlp_dims: dict, hidden_dims: dict,\n",
    "                 output_dims: dict, act_fn_in: nn.ReLU(), act_fn_out: nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.out_dim_X = output_dims['X']\n",
    "        self.out_dim_E = output_dims['E']\n",
    "        self.out_dim_y = output_dims['y']\n",
    "\n",
    "        self.mlp_in_X = nn.Sequential(nn.Linear(input_dims['X'], hidden_mlp_dims['X']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['X'], hidden_dims['dx']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_E = nn.Sequential(nn.Linear(input_dims['E'], hidden_mlp_dims['E']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['E'], hidden_dims['de']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_y = nn.Sequential(nn.Linear(input_dims['y'], hidden_mlp_dims['y']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['y'], hidden_dims['dy']), act_fn_in)\n",
    "\n",
    "        self.tf_layers = nn.ModuleList([XEyTransformerLayer(dx=hidden_dims['dx'],\n",
    "                                                            de=hidden_dims['de'],\n",
    "                                                            dy=hidden_dims['dy'],\n",
    "                                                            n_head=hidden_dims['n_head'],\n",
    "                                                            dim_ffX=hidden_dims['dim_ffX'],\n",
    "                                                            dim_ffE=hidden_dims['dim_ffE'])\n",
    "                                        for i in range(n_layers)])\n",
    "\n",
    "        self.mlp_out_X = nn.Sequential(nn.Linear(hidden_dims['dx'], hidden_mlp_dims['X']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['X'], output_dims['X']))\n",
    "\n",
    "        self.mlp_out_E = nn.Sequential(nn.Linear(hidden_dims['de'], hidden_mlp_dims['E']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['E'], output_dims['E']))\n",
    "\n",
    "        self.mlp_out_y = nn.Sequential(nn.Linear(hidden_dims['dy'], hidden_mlp_dims['y']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['y'], output_dims['y']))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        bs, n = X.shape[0], X.shape[1]\n",
    "\n",
    "        diag_mask = torch.eye(n)\n",
    "        diag_mask = ~diag_mask.type_as(E).bool()\n",
    "        diag_mask = diag_mask.unsqueeze(0).unsqueeze(-1).expand(bs, -1, -1, -1)\n",
    "\n",
    "        X_to_out = X[..., :self.out_dim_X]\n",
    "        E_to_out = E[..., :self.out_dim_E]\n",
    "        y_to_out = y[..., :self.out_dim_y]\n",
    "\n",
    "        new_E = self.mlp_in_E(E)\n",
    "        new_E = (new_E + new_E.transpose(1, 2)) / 2\n",
    "        logging.debug(f\"X shape: {X.shape}\")\n",
    "        after_in = utils.PlaceHolder(X=self.mlp_in_X(X), E=new_E, y=self.mlp_in_y(y)).mask(node_mask)\n",
    "        logging.debug(f\"after_in.X shape: {after_in.X.shape}\")\n",
    "        X, E, y = after_in.X, after_in.E, after_in.y\n",
    "\n",
    "        for layer in self.tf_layers:\n",
    "            X, E, y = layer(X, E, y, node_mask)\n",
    "\n",
    "        X = self.mlp_out_X(X)\n",
    "        E = self.mlp_out_E(E)\n",
    "        y = self.mlp_out_y(y)\n",
    "\n",
    "        X = (X + X_to_out)\n",
    "        E = (E + E_to_out) * diag_mask\n",
    "        y = y + y_to_out\n",
    "\n",
    "        E = 1/2 * (E + torch.transpose(E, 1, 2))\n",
    "\n",
    "        return utils.PlaceHolder(X=X, E=E, y=y).mask(node_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
