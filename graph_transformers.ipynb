{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Transformer Model\n",
    "\n",
    "DiGress trained a graph transformer network proposed by Dwivedi & Bresson (2021) s the denoising model. In this section, we will go through the implementation details of the model to see how it predicts the clean graph from the noisy graph. \n",
    "\n",
    "\n",
    "- **$\\mathbf{X}$**: Node features matrix, shape ( bs, $n, d_x$ )\n",
    "- **$\\mathbf{E}$**: Edge features matrix, shape (bs, $n, n, d_e$ )\n",
    "- **$\\mathbf{y}$**: Global features vector, shape (bs, $d_y$ )\n",
    "- **node_mask**: Node mask, shape (bs, $n$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch import Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def assert_correctly_masked(variable, node_mask):\n",
    "    assert (variable * (1 - node_mask.long())).abs().max().item() < 1e-4, \\\n",
    "        'Variables not masked properly.'\n",
    "\n",
    "def masked_softmax(x, mask, **kwargs):\n",
    "    if mask.sum() == 0:\n",
    "        return x\n",
    "    x_masked = x.clone()\n",
    "    x_masked[mask == 0] = -float(\"inf\")\n",
    "    return torch.softmax(x_masked, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Node/Edge Features($\\mathbf X, \\mathbf E$) to Global Features $y$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname{PNA}(\\boldsymbol{X})=\\operatorname{cat}(\\max (\\boldsymbol{X}), \\min (\\boldsymbol{X}), \\operatorname{mean}(\\boldsymbol{X}), \\operatorname{std}(\\boldsymbol{X})) \\boldsymbol{W}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xtoy(nn.Module):\n",
    "    def __init__(self, dx, dy):\n",
    "        \"\"\" Map node features to global features \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * dx, dy)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" X: bs, n, dx. \"\"\"\n",
    "        m = X.mean(dim=1)     # bs, dx\n",
    "        mi = X.min(dim=1)[0]  # bs, dx\n",
    "        ma = X.max(dim=1)[0]  # bs, dx\n",
    "        std = X.std(dim=1)    # bs, dx\n",
    "        z = torch.hstack((m, mi, ma, std)) # bs, 4 * dx\n",
    "        out = self.lin(z)    # bs, dy\n",
    "        return out\n",
    "\n",
    "\n",
    "class Etoy(nn.Module):\n",
    "    def __init__(self, d, dy):\n",
    "        \"\"\" Map edge features to global features. \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * d, dy)\n",
    "\n",
    "    def forward(self, E):\n",
    "        \"\"\" E: bs, n, n, de\n",
    "            Features relative to the diagonal of E could potentially be added.\n",
    "        \"\"\"\n",
    "        m = E.mean(dim=(1, 2))              # bs, de\n",
    "        mi = E.min(dim=2)[0].min(dim=1)[0]  # bs, de\n",
    "        ma = E.max(dim=2)[0].max(dim=1)[0]  # bs, de\n",
    "        std = torch.std(E, dim=(1, 2))      # bs, de\n",
    "        z = torch.hstack((m, mi, ma, std))  # bs, 4 * de\n",
    "        out = self.lin(z)                   # bs, dy\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention `NodeEdgeBlock`\n",
    "Self-attention layer that updates the representations on the node, edges, and global features\n",
    "$$\\mathbf{X}_{\\text {new }}, \\mathbf{E}_{\\text {new}}, \\mathbf{y}_{\\text {new}}=\\operatorname{SelfAttn}\\left(\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text{node}_\\text{mask}\\right)$$\n",
    "\n",
    "### 1. Linear Projections and Maskings\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\mathbf{Q}=\\mathbf{X} \\mathbf{W}_Q \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{K}=\\mathbf{X} \\mathbf{W}_K \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{V}=\\mathbf{X} \\mathbf{W}_V \\odot \\mathbf{x}_{\\text {mask }}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### 2. Reshape $\\mathbf{Q,K,V}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "   & \\mathbf Q = \\mathbf Q. \\textrm{reshape(bs, n, nhead, df)} \\\\\n",
    "   & \\mathbf K = \\mathbf K. \\textrm{reshape(bs, n, nhead, df)} \\\\\n",
    "   & \\mathbf V = \\mathbf V. \\textrm{reshape(bs, n, nhead, df)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### 2. Calculate the Attention Score \n",
    "The query, key, and value matrices are transformed into multi-head\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf Y = \\frac{(\\mathbf Q \\times \\mathbf K^T)}{\\sqrt{\\textrm{df}}}\n",
    "\\end{equation}\n",
    "\n",
    "### 3. FiLM Layers\n",
    "\n",
    "\n",
    "#### (a) Incorporate Edge Features to the self-attention score\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbf{E_1} & =\\left( \\mathbf W^{\\textrm{emul}}_E \\mathbf{E} \\times \\mathbf{e_{\\textrm{mask}}} ).\\textrm{reshape}(\\mathrm{bs}, \\mathrm{n}, \\mathrm{n}, \\mathrm{n} \\text { head, df })\\right. \\\\\n",
    "\\mathbf{E_2} & =\\left( \\mathbf W^{\\textrm{add}}_E \\mathbf{E} \\times \\mathbf{e_{\\textrm{mask}}} ).\\textrm{reshape}(\\mathrm{bs}, \\mathrm{n}, \\mathrm{n}, \\mathrm{n} \\text { head, df })\\right. \\\\\n",
    "\\mathbf{Y} & =\\mathbf{Y} \\times(\\mathbf{E} \\mathbf{1}+1)+\\mathbf{E} \\mathbf{2}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Incorporate $y$ to $\\mathbf E$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&  E_{\\textrm{new}} = \\mathbf Y.\\textrm{reshape}(\\textrm{bs, n, n, dx}) \\\\\n",
    "& \\mathbf E_{\\textrm{out}} = W^{\\textrm{add}}_{ye}y +  (W^{\\textrm{mul}}_{ye}y +1 ) \\times \\textrm{new}\\mathbf E \\times \\mathbf{e_{\\textrm{mask}}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### 4. Compute Normalized Attention Scores \n",
    "\n",
    "\\begin{equation}\n",
    "    \\textrm{Attn} = \\textrm{softmax} (\\mathbf Y \\times \\textrm{softmax}_{\\textrm{mask}}) \\in \\mathbb R^{bs \\times n\\times n \\times \\textrm{nhead}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute Weighted Values \n",
    "This step aggregates information from connected nodes weighted by the computed attention scores, effectively updating node representations based on their neighborhood.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf V_{\\textrm{weighted}} = \\sum_i \\textrm{Attn} \\times \\mathbf V \\in \\mathbb R^{bs \\times n \\times dx}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Update Representations\n",
    "Node, edge, and global feature representations are updated through additional FiLM layers and linear transformations:\n",
    "\n",
    "#### (a) Incorporate $y$ to $\\mathbf X$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    &  \\mathbf X_{\\textrm{new}} = W_{yx}^{\\textrm{add}} + (W_{yx}^{\\textrm{mul}} + 1) \\times \\mathbf V_{\\textrm{weighted}} \\\\\n",
    "    &  \\mathbf X_{\\textrm{out}} = \\mathbf X_{\\textrm{new}} W_{xx} \\odot \\mathbf x_{\\textrm{mask}}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### (b) Process $y$ based on $\\mathbf{X, E}$\n",
    "\n",
    "\\begin{equation}\n",
    "    y_{\\textrm{out}} = (yW_{yy} + \\mathbf E W_{ey} + X W_{xy}) W_{yy}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEdgeBlock(nn.Module):\n",
    "    def __init__(self, dx, de, dy, n_head, **kwargs):\n",
    "        super().__init__()\n",
    "        assert dx % n_head == 0, f\"dx: {dx} -- nhead: {n_head}\"\n",
    "        self.dx = dx\n",
    "        self.de = de\n",
    "        self.dy = dy\n",
    "        self.df = int(dx / n_head)\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # Attention\n",
    "        self.q = Linear(dx, dx)\n",
    "        self.k = Linear(dx, dx)\n",
    "        self.v = Linear(dx, dx)\n",
    "        # FiLM E to X\n",
    "        self.e_add = Linear(de, dx)\n",
    "        self.e_mul = Linear(de, dx)\n",
    "        # FiLM y to E\n",
    "        self.y_e_mul = Linear(dy, dx)           # Warning: here it's dx and not de\n",
    "        self.y_e_add = Linear(dy, dx)\n",
    "        # FiLM y to X\n",
    "        self.y_x_mul = Linear(dy, dx)\n",
    "        self.y_x_add = Linear(dy, dx)\n",
    "        # Process y\n",
    "        self.y_y = Linear(dy, dy)\n",
    "        self.x_y = Xtoy(dx, dy)\n",
    "        self.e_y = Etoy(de, dy)\n",
    "        # Output layers\n",
    "        self.x_out = Linear(dx, dx)\n",
    "        self.e_out = Linear(dx, de)\n",
    "        self.y_out = nn.Sequential(nn.Linear(dy, dy), nn.ReLU(), nn.Linear(dy, dy))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        \"\"\"\n",
    "        :param X: bs, n, d        node features\n",
    "        :param E: bs, n, n, d     edge features\n",
    "        :param y: bs, dy           global features\n",
    "        :param node_mask: bs, n\n",
    "        :return: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "        bs, n, _ = X.shape\n",
    "        x_mask = node_mask.unsqueeze(-1)        # bs, n, 1\n",
    "        e_mask1 = x_mask.unsqueeze(2)           # bs, n, 1, 1\n",
    "        e_mask2 = x_mask.unsqueeze(1)           # bs, 1, n, 1\n",
    "\n",
    "        # 1. Map X to keys and queries\n",
    "        Q = self.q(X) * x_mask                  # (bs, n, dx)\n",
    "        K = self.k(X) * x_mask           \n",
    "        V = self.v(X) * x_mask               \n",
    "        assert_correctly_masked(Q, x_mask)\n",
    "        \n",
    "        # 2. Reshape to (bs, n, n_head, df) with dx = n_head * df\n",
    "        Q = Q.reshape((Q.size(0), Q.size(1), self.n_head, self.df))\n",
    "        K = K.reshape((K.size(0), K.size(1), self.n_head, self.df))\n",
    "        V = V.reshape((V.size(0), V.size(1), self.n_head, self.df))\n",
    "\n",
    "        Q = Q.unsqueeze(2)                              # (bs, 1, n, n_head, df)\n",
    "        K = K.unsqueeze(1)                              # (bs, n, 1, n head, df)\n",
    "        V = V.unsqueeze(1)                              # (bs, 1, n, n_head, df)\n",
    "\n",
    "        # Compute unnormalized attentions. Y is (bs, n, n, n_head, df)\n",
    "        Y = Q * K\n",
    "        Y = Y / math.sqrt(Y.size(-1)) \n",
    "\n",
    "        assert_correctly_masked(Y, (e_mask1 * e_mask2).unsqueeze(-1))\n",
    "\n",
    "        E1 = self.e_mul(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E1 = E1.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df)) # bs, n, n, n_head, df\n",
    "\n",
    "\n",
    "        E2 = self.e_add(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E2 = E2.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))\n",
    "\n",
    "        # Incorporate edge features to the self attention scores.\n",
    "        Y = Y * (E1 + 1) + E2                  # (bs, n, n, n_head, df)\n",
    "\n",
    "        # Incorporate y to E\n",
    "        newE = Y.flatten(start_dim=3)                    # bs, n, n, dx\n",
    "        ye1 = self.y_e_add(y).unsqueeze(1).unsqueeze(1)  # bs, 1, 1, de\n",
    "        ye2 = self.y_e_mul(y).unsqueeze(1).unsqueeze(1)\n",
    "        newE = ye1 + (ye2 + 1) * newE\n",
    "\n",
    "        # Output E\n",
    "        newE = self.e_out(newE) * e_mask1 * e_mask2      # bs, n, n, de\n",
    "        assert_correctly_masked(newE, e_mask1 * e_mask2)\n",
    "\n",
    "        # Compute attentions. attn is still (bs, n, n, n_head, df)\n",
    "        softmax_mask = e_mask2.expand(-1, n, -1, self.n_head)    # bs, 1, n, 1\n",
    "        attn = masked_softmax(Y, softmax_mask, dim=2)  # bs, n, n, n_head\n",
    "\n",
    "        # Compute weighted values\n",
    "        weighted_V = attn * V                          # bs, n, n, n_head, df\n",
    "        weighted_V = weighted_V.sum(dim=2)             # bs, n, n_head, df\n",
    "\n",
    "        # Send output to input dim\n",
    "        weighted_V = weighted_V.flatten(start_dim=2)            # bs, n, dx\n",
    "\n",
    "        # Incorporate y to X\n",
    "        yx1 = self.y_x_add(y).unsqueeze(1)\n",
    "        yx2 = self.y_x_mul(y).unsqueeze(1)\n",
    "        newX = yx1 + (yx2 + 1) * weighted_V\n",
    "\n",
    "        # Output X\n",
    "        newX = self.x_out(newX) * x_mask\n",
    "        assert_correctly_masked(newX, x_mask)\n",
    "\n",
    "        # Process y based on X and E\n",
    "        y = self.y_y(y)\n",
    "        e_y = self.e_y(E)\n",
    "        x_y = self.x_y(X)\n",
    "        new_y = y + x_y + e_y\n",
    "        new_y = self.y_out(new_y)               # bs, dy\n",
    "\n",
    "        return newX, newE, new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 10, 64])\n",
      "torch.Size([2, 10, 10, 8, 8])\n",
      "Shape of newX: torch.Size([2, 10, 64])\n",
      "Shape of newE: torch.Size([2, 10, 10, 32])\n",
      "Shape of new_y: torch.Size([2, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example parameters for the NodeEdgeBlock\n",
    "dx = 64\n",
    "de = 32\n",
    "dy = 16\n",
    "n_head = 8\n",
    "bs = 2  # batch size\n",
    "n = 10  # number of nodes\n",
    "\n",
    "# Initialize the NodeEdgeBlock\n",
    "node_edge_block = NodeEdgeBlock(dx, de, dy, n_head)\n",
    "\n",
    "# Create example input tensors\n",
    "X = torch.randn(bs, n, dx)\n",
    "E = torch.randn(bs, n, n, de)\n",
    "y = torch.randn(bs, dy)\n",
    "node_mask = torch.ones(bs, n)  # example mask where all nodes are valid\n",
    "\n",
    "# Forward pass\n",
    "newX, newE, new_y = node_edge_block(X, E, y, node_mask)\n",
    "\n",
    "# Print shapes of the output tensors\n",
    "print(\"Shape of newX:\", newX.shape)\n",
    "print(\"Shape of newE:\", newE.shape)\n",
    "print(\"Shape of new_y:\", new_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer that updates node, edge and global features\n",
    "- **$\\mathbf{X}$**: Node features matrix, shape ( bs, $n, d_x$ )\n",
    "- **$\\mathbf{E}$**: Edge features matrix, shape (bs, $n, n, d_e$ )\n",
    "- **$\\mathbf{y}$**: Global features vector, shape (bs, $d_y$ )\n",
    "- **node_mask**: Node mask, shape (bs, $n$)\n",
    "- **$\\mathbf{W}$**: Weight matrices for the linear layers\n",
    "- **$\\mathbf{b}$**: Bias vectors for the linear layers\n",
    "- **LN**: Layer normalization ...\n",
    "- **Dropout**: Dropout operation ...\n",
    "- **ReLU**: Rectified Linear Unit activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XEyTransformerLayer(nn.Module):\n",
    "    \"\"\" Transformer that updates node, edge and global features\n",
    "        d_x: node features\n",
    "        d_e: edge features\n",
    "        dz : global features\n",
    "        n_head: the number of heads in the multi_head_attention\n",
    "        dim_feedforward: the dimension of the feedforward network model after self-attention\n",
    "        dropout: dropout probablility. 0 to disable\n",
    "        layer_norm_eps: eps value in layer normalizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,\n",
    "                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,\n",
    "                 layer_norm_eps: float = 1e-5, device=None, dtype=None) -> None:\n",
    "        kw = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = NodeEdgeBlock(dx, de, dy, n_head, **kw)\n",
    "\n",
    "        self.linX1 = Linear(dx, dim_ffX, **kw)\n",
    "        self.linX2 = Linear(dim_ffX, dx, **kw)\n",
    "        self.normX1 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.normX2 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutX1 = Dropout(dropout)\n",
    "        self.dropoutX2 = Dropout(dropout)\n",
    "        self.dropoutX3 = Dropout(dropout)\n",
    "\n",
    "        self.linE1 = Linear(de, dim_ffE, **kw)\n",
    "        self.linE2 = Linear(dim_ffE, de, **kw)\n",
    "        self.normE1 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.normE2 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutE1 = Dropout(dropout)\n",
    "        self.dropoutE2 = Dropout(dropout)\n",
    "        self.dropoutE3 = Dropout(dropout)\n",
    "\n",
    "        self.lin_y1 = Linear(dy, dim_ffy, **kw)\n",
    "        self.lin_y2 = Linear(dim_ffy, dy, **kw)\n",
    "        self.norm_y1 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.norm_y2 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.dropout_y1 = Dropout(dropout)\n",
    "        self.dropout_y2 = Dropout(dropout)\n",
    "        self.dropout_y3 = Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, X: Tensor, E: Tensor, y, node_mask: Tensor):\n",
    "        \"\"\" Pass the input through the encoder layer.\n",
    "            X: (bs, n, d)\n",
    "            E: (bs, n, n, d)\n",
    "            y: (bs, dy)\n",
    "            node_mask: (bs, n) Mask for the src keys per batch (optional)\n",
    "            Output: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "        newX, newE, new_y = self.self_attn(X, E, y, node_mask=node_mask)\n",
    "\n",
    "        newX_d = self.dropoutX1(newX)\n",
    "        X = self.normX1(X + newX_d)\n",
    "\n",
    "        newE_d = self.dropoutE1(newE)\n",
    "        E = self.normE1(E + newE_d)\n",
    "\n",
    "        new_y_d = self.dropout_y1(new_y)\n",
    "        y = self.norm_y1(y + new_y_d)\n",
    "\n",
    "        ff_outputX = self.linX2(self.dropoutX2(self.activation(self.linX1(X))))\n",
    "        ff_outputX = self.dropoutX3(ff_outputX)\n",
    "        X = self.normX2(X + ff_outputX)\n",
    "\n",
    "        ff_outputE = self.linE2(self.dropoutE2(self.activation(self.linE1(E))))\n",
    "        ff_outputE = self.dropoutE3(ff_outputE)\n",
    "        E = self.normE2(E + ff_outputE)\n",
    "\n",
    "        ff_output_y = self.lin_y2(self.dropout_y2(self.activation(self.lin_y1(y))))\n",
    "        ff_output_y = self.dropout_y3(ff_output_y)\n",
    "        y = self.norm_y2(y + ff_output_y)\n",
    "\n",
    "        return X, E, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XEyTransformerLayer\n",
    "\n",
    "```python\n",
    "def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,\n",
    "                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,\n",
    "                 layer_norm_eps: float = 1e-5)\n",
    "```\n",
    "\n",
    "Transformer that updates node, edge and global features\n",
    "- **$\\mathbf{X}$**: Node features matrix, shape ( bs, $n, d_x$ )\n",
    "- **$\\mathbf{E}$**: Edge features matrix, shape (bs, $n, n, d_e$ )\n",
    "- **$\\mathbf{y}$**: Global features vector, shape (bs, $d_y$ )\n",
    "- **node_mask**: Node mask, shape (bs, $n$ )\n",
    "- **$\\mathbf{W}$**: Weight matrices for the linear layers\n",
    "- **$\\mathbf{b}$**: Bias vectors for the linear layers\n",
    "- **LN**: Layer normalization\n",
    "- **Dropout**: Dropout operation\n",
    "- **ReLU**: Rectified Linear Unit activation function\n",
    "\n",
    "#### Self Attention\n",
    "\n",
    "$$\\mathbf{X}_{\\text {new }}, \\mathbf{E}_{\\text {new}}, \\mathbf{y}_{\\text {new}}=\\operatorname{SelfAttn}\\left(\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text{node}_\\text{mask}\\right)$$\n",
    "\n",
    "\n",
    "#### Residual and Layer Normalization \n",
    "\n",
    "\\begin{gathered}\n",
    "\\mathbf{X}_{\\text {residual }}=\\mathbf{X}+\\operatorname{Dropout}\\left(\\mathbf{X}_{\\text {new }}\\right) \\\\\n",
    "\\mathbf{X}=\\operatorname{LN}\\left(\\mathbf{X}_{\\text {residual }}\\right)\n",
    "\\end{gathered}\n",
    "\n",
    "\n",
    "####  Feed-Forward Layer \n",
    "\n",
    "\\begin{gathered}\n",
    "\\mathbf{X}_{\\mathrm{ff}}=\\operatorname{Dropout}\\left(\\operatorname{ReLU}\\left(\\mathbf{X} \\mathbf{W}_{X 1}+\\mathbf{b}_{X 1}\\right)\\right) \\\\\n",
    "\\mathbf{X}_{\\mathrm{ff}}=\\mathbf{X}_{\\mathrm{ff}} \\mathbf{W}_{X 2}+\\mathbf{b}_{X 2} \\\\\n",
    "\\mathbf{X}_{\\mathrm{ff}}=\\operatorname{Dropout}\\left(\\mathbf{X}_{\\mathrm{ff}}\\right) \\\\\n",
    "\\mathbf{X}=\\mathrm{LN}\\left(\\mathbf{X}+\\mathbf{X}_{\\mathrm{ff}}\\right)\n",
    "\\end{gathered}\n",
    "\n",
    "\n",
    "### NodeEdgeBlock\n",
    "Self-attention layer that also updates the representations on the edges\n",
    "\n",
    "```python\n",
    "def __init__(self, dx, de, dy, n_head, **kwargs):\n",
    "```\n",
    "\n",
    "#### Linear Projections and Maskings \n",
    "\n",
    "\\begin{aligned}\n",
    "& \\mathbf{Q}=\\mathbf{X} \\mathbf{W}_Q \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{K}=\\mathbf{X} \\mathbf{W}_K \\odot \\mathbf{x}_{\\text {mask }} \\\\\n",
    "& \\mathbf{V}=\\mathbf{X} \\mathbf{W}_V \\odot \\mathbf{x}_{\\text {mask }}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "#### Reshape for Multi-head Attention\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\mathbf{Q} = \\operatorname{reshape}\\left(\\mathbf{Q}, \\left(\\mathrm{bs}, n, \\mathrm{n_head}, \\mathrm{df}\\right)\\right) \\\\\n",
    "&\\mathbf{K} = \\operatorname{reshape}\\left(\\mathbf{K}, \\left(\\mathrm{bs}, n, \\mathrm{n_head}, \\mathrm{df}\\right)\\right) \\\\\n",
    "&\\mathbf{V} = \\operatorname{reshape}\\left(\\mathbf{V}, \\left(\\mathrm{bs}, n, \\mathrm{n_head}, \\mathrm{df}\\right)\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "#### Compute Unnormalized Attention Scores\n",
    "\n",
    "$$\\mathbf{Y} = \\frac{(\\mathbf{Q} \\odot \\mathbf{x}_{\\text{mask}}) \\cdot (\\mathbf{K} \\odot \\mathbf{x}_{\\text{mask}})^{\\top}}{\\sqrt{\\mathrm{df}}}$$\n",
    "\n",
    "\n",
    "#### Incorporate Edge Features \n",
    "\n",
    "\\begin{gathered}\n",
    "\\mathbf{E}_{\\text {mul }}=\\operatorname{reshape}\\left(\\mathbf{E} \\mathbf{W}_{\\text {e_mul }},\\left(\\mathrm{bs}, n, n, \\mathrm{n} _ \\text {head, df }\\right)\\right) \\odot \\mathbf{e}_{\\text {mask1 }} \\odot \\mathbf{e}_{\\text {mask2 }} \\\\\n",
    "\\mathbf{E}_{\\text {add }}=\\operatorname{reshape}\\left(\\mathbf{E} \\mathbf{W}_{\\text {e_add }},\\left(\\mathrm{bs}, n, n, \\mathrm{n} _ \\text {head, df }\\right)\\right) \\odot \\mathbf{e}_{\\text {mask1 }} \\odot \\mathbf{e}_{\\text {mask2 }} \\\\\n",
    "\\mathbf{Y}=\\mathbf{Y} \\odot\\left(\\mathbf{E}_{\\text {mul }}+1\\right)+\\mathbf{E}_{\\text {add }}\n",
    "\\end{gathered}\n",
    "\n",
    "\n",
    "#### Compute Weighted Values\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\mathbf{V} = \\operatorname{reshape}(\\mathbf{V}, (\\text{bs}, 1, n, \\text{n_head}, \\text{df})) \\odot \\mathbf{x}_{\\text{mask}} \\quad (\\text{expand mask}) \\\\\n",
    "& \\mathbf{weighted_V} = \\sum_{j} \\mathbf{A}_{ij} \\mathbf{V}_{j} \\\\\n",
    "& \\mathbf{weighted_V} = \\operatorname{reshape}(\\mathbf{weighted_V}, (\\text{bs}, n, d_x))\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "#### Incorporate Global Features to Node Features\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\mathbf{y}_{\\text{x_add}} = \\mathbf{y} \\mathbf{W}_{\\text{y_x_add}} \\odot \\mathbf{x}_{\\text{mask}} \\\\\n",
    "& \\mathbf{y}_{\\text{x_mul}} = \\mathbf{y} \\mathbf{W}_{\\text{y_x_mul}} \\odot \\mathbf{x}_{\\text{mask}} \\\\\n",
    "& \\mathbf{newX} = \\mathbf{y}_{\\text{x_add}} + (\\mathbf{y}_{\\text{x_mul}} + 1) \\odot \\mathbf{weighted_V}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "#### Output Node Features\n",
    "\n",
    "$$\\mathbf{newX} = \\mathbf{newX} \\mathbf{W}_{\\text{x_out}} \\odot \\mathbf{x}_{\\text{mask}}$$\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "The update of the node features $\\mathbf{X}$ in the `NodeEdgeBlock` involves:\n",
    "1. Projecting $\\mathbf{X}$ to queries $\\mathbf{Q}$, keys $\\mathbf{K}$, and values $\\mathbf{V}$.\n",
    "2. Computing scaled dot-product attention scores.\n",
    "3. Incorporating edge features into the attention scores.\n",
    "4. Applying a masked softmax to obtain attention weights.\n",
    "5. Computing weighted sums of values based on the attention weights.\n",
    "6. Incorporating global features into the updated node features.\n",
    "7. Producing the final updated node features.\n",
    "\n",
    "### GraphTransformer \n",
    "\n",
    "The `GraphTransformer` class combines multi-layer perceptrons (MLPs) and transformer layers to process graph data. \n",
    "\n",
    "1. **Initial Feature Processing:**\n",
    "\n",
    "   \\begin{aligned}\n",
    "   &\\mathbf{X}_{\\text{in}} = \\sigma(\\mathbf{W}_{1X} \\sigma(\\mathbf{W}_{0X} \\mathbf{X} + \\mathbf{b}_{0X}) + \\mathbf{b}_{1X}) \\\\\n",
    "   &\\mathbf{E}_{\\text{in}} = \\frac{1}{2} (\\sigma(\\mathbf{W}_{1E} \\sigma(\\mathbf{W}_{0E} \\mathbf{E} + \\mathbf{b}_{0E}) + \\mathbf{b}_{1E}) + \\sigma(\\mathbf{W}_{1E} \\sigma(\\mathbf{W}_{0E} \\mathbf{E}^T + \\mathbf{b}_{0E}) + \\mathbf{b}_{1E})) \\\\\n",
    "   &\\mathbf{y}_{\\text{in}} = \\sigma(\\mathbf{W}_{1y} \\sigma(\\mathbf{W}_{0y} \\mathbf{y} + \\mathbf{b}_{0y}) + \\mathbf{b}_{1y}) \\\\\n",
    "   \\end{aligned}\n",
    "\n",
    "\n",
    "2. **Transformer Layers:**\n",
    "   For each transformer layer $i$:\n",
    "\n",
    "   $$\\mathbf{X}, \\mathbf{E}, \\mathbf{y} = \\text{tf_layers}[i](\\mathbf{X}, \\mathbf{E}, \\mathbf{y}, \\text{node_mask})$$\n",
    "\n",
    "\n",
    "3. **Final Feature Processing:**\n",
    "\n",
    "\n",
    "\n",
    "   \\begin{aligned}\n",
    "   &\\mathbf{X}_{\\text{out}} = \\sigma(\\mathbf{W}_{3X} \\sigma(\\mathbf{W}_{2X} \\mathbf{X} + \\mathbf{b}_{2X}) + \\mathbf{b}_{3X}) + \\mathbf{X} \\\\\n",
    "   &\\mathbf{E}_{\\text{out}} = \\sigma(\\mathbf{W}_{3E} \\sigma(\\mathbf{W}_{2E} \\mathbf{E} + \\mathbf{b}_{2E}) + \\mathbf{b}_{3E}) \\cdot \\text{diag_mask} + \\mathbf{E} \\cdot \\text{diag_mask} \\\\\n",
    "   &\\mathbf{y}_{\\text{out}} = \\sigma(\\mathbf{W}_{3y} \\sigma(\\mathbf{W}_{2y} \\mathbf{y} + \\mathbf{b}_{2y}) + \\mathbf{b}_{3y}) + \\mathbf{y} \\\\\n",
    "   \\end{aligned}\n",
    "\n",
    "\n",
    "4. **Symmetrizing Edge Features:**\n",
    "\n",
    "   $$\\mathbf{E}_{\\text{out}} = \\frac{1}{2} (\\mathbf{E}_{\\text{out}} + \\mathbf{E}_{\\text{out}}^T)$$\n",
    "\n",
    "\n",
    "5. **Return Masked Outputs:**\n",
    "\n",
    "   $$\\text{utils.PlaceHolder}(\\mathbf{X} = \\mathbf{X}_{\\text{out}}, \\mathbf{E} = \\mathbf{E}_{\\text{out}}, \\mathbf{y} = \\mathbf{y}_{\\text{out}}).mask(\\text{node_mask})$$\n",
    "\n",
    "\n",
    "### Summary\n",
    "The `GraphTransformer` processes node (\\(\\mathbf{X}\\)), edge (\\(\\mathbf{E}\\)), and global (\\(\\mathbf{y}\\)) features through MLPs, applies multiple transformer layers to update the features, and then processes the updated features through output MLPs before returning the final results with symmetric edge features and masked nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class XEyTransformerLayer(nn.Module):\n",
    "    \"\"\" Transformer that updates node, edge and global features\n",
    "        d_x: node features\n",
    "        d_e: edge features\n",
    "        dz : global features\n",
    "        n_head: the number of heads in the multi_head_attention\n",
    "        dim_feedforward: the dimension of the feedforward network model after self-attention\n",
    "        dropout: dropout probablility. 0 to disable\n",
    "        layer_norm_eps: eps value in layer normalizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,\n",
    "                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,\n",
    "                 layer_norm_eps: float = 1e-5, device=None, dtype=None) -> None:\n",
    "        kw = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = NodeEdgeBlock(dx, de, dy, n_head, **kw)\n",
    "\n",
    "        self.linX1 = Linear(dx, dim_ffX, **kw)\n",
    "        self.linX2 = Linear(dim_ffX, dx, **kw)\n",
    "        self.normX1 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.normX2 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutX1 = Dropout(dropout)\n",
    "        self.dropoutX2 = Dropout(dropout)\n",
    "        self.dropoutX3 = Dropout(dropout)\n",
    "\n",
    "        self.linE1 = Linear(de, dim_ffE, **kw)\n",
    "        self.linE2 = Linear(dim_ffE, de, **kw)\n",
    "        self.normE1 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.normE2 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutE1 = Dropout(dropout)\n",
    "        self.dropoutE2 = Dropout(dropout)\n",
    "        self.dropoutE3 = Dropout(dropout)\n",
    "\n",
    "        self.lin_y1 = Linear(dy, dim_ffy, **kw)\n",
    "        self.lin_y2 = Linear(dim_ffy, dy, **kw)\n",
    "        self.norm_y1 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.norm_y2 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.dropout_y1 = Dropout(dropout)\n",
    "        self.dropout_y2 = Dropout(dropout)\n",
    "        self.dropout_y3 = Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, X: Tensor, E: Tensor, y, node_mask: Tensor):\n",
    "        \"\"\" Pass the input through the encoder layer.\n",
    "            X: (bs, n, d)\n",
    "            E: (bs, n, n, d)\n",
    "            y: (bs, dy)\n",
    "            node_mask: (bs, n) Mask for the src keys per batch (optional)\n",
    "            Output: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "\n",
    "        newX, newE, new_y = self.self_attn(X, E, y, node_mask=node_mask)\n",
    "\n",
    "        newX_d = self.dropoutX1(newX)\n",
    "        X = self.normX1(X + newX_d)\n",
    "\n",
    "        newE_d = self.dropoutE1(newE)\n",
    "        E = self.normE1(E + newE_d)\n",
    "\n",
    "        new_y_d = self.dropout_y1(new_y)\n",
    "        y = self.norm_y1(y + new_y_d)\n",
    "\n",
    "        ff_outputX = self.linX2(self.dropoutX2(self.activation(self.linX1(X))))\n",
    "        ff_outputX = self.dropoutX3(ff_outputX)\n",
    "        X = self.normX2(X + ff_outputX)\n",
    "\n",
    "        ff_outputE = self.linE2(self.dropoutE2(self.activation(self.linE1(E))))\n",
    "        ff_outputE = self.dropoutE3(ff_outputE)\n",
    "        E = self.normE2(E + ff_outputE)\n",
    "\n",
    "        ff_output_y = self.lin_y2(self.dropout_y2(self.activation(self.lin_y1(y))))\n",
    "        ff_output_y = self.dropout_y3(ff_output_y)\n",
    "        y = self.norm_y2(y + ff_output_y)\n",
    "\n",
    "        return X, E, y\n",
    "\n",
    "\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    n_layers : int -- number of layers\n",
    "    dims : dict -- contains dimensions for each feature type\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers: int, input_dims: dict, hidden_mlp_dims: dict, hidden_dims: dict,\n",
    "                 output_dims: dict, act_fn_in: nn.ReLU(), act_fn_out: nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.out_dim_X = output_dims['X']\n",
    "        self.out_dim_E = output_dims['E']\n",
    "        self.out_dim_y = output_dims['y']\n",
    "\n",
    "        self.mlp_in_X = nn.Sequential(nn.Linear(input_dims['X'], hidden_mlp_dims['X']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['X'], hidden_dims['dx']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_E = nn.Sequential(nn.Linear(input_dims['E'], hidden_mlp_dims['E']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['E'], hidden_dims['de']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_y = nn.Sequential(nn.Linear(input_dims['y'], hidden_mlp_dims['y']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['y'], hidden_dims['dy']), act_fn_in)\n",
    "\n",
    "        self.tf_layers = nn.ModuleList([XEyTransformerLayer(dx=hidden_dims['dx'],\n",
    "                                                            de=hidden_dims['de'],\n",
    "                                                            dy=hidden_dims['dy'],\n",
    "                                                            n_head=hidden_dims['n_head'],\n",
    "                                                            dim_ffX=hidden_dims['dim_ffX'],\n",
    "                                                            dim_ffE=hidden_dims['dim_ffE'])\n",
    "                                        for i in range(n_layers)])\n",
    "\n",
    "        self.mlp_out_X = nn.Sequential(nn.Linear(hidden_dims['dx'], hidden_mlp_dims['X']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['X'], output_dims['X']))\n",
    "\n",
    "        self.mlp_out_E = nn.Sequential(nn.Linear(hidden_dims['de'], hidden_mlp_dims['E']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['E'], output_dims['E']))\n",
    "\n",
    "        self.mlp_out_y = nn.Sequential(nn.Linear(hidden_dims['dy'], hidden_mlp_dims['y']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['y'], output_dims['y']))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        bs, n = X.shape[0], X.shape[1]\n",
    "\n",
    "        diag_mask = torch.eye(n)\n",
    "        diag_mask = ~diag_mask.type_as(E).bool()\n",
    "        diag_mask = diag_mask.unsqueeze(0).unsqueeze(-1).expand(bs, -1, -1, -1)\n",
    "\n",
    "        X_to_out = X[..., :self.out_dim_X]\n",
    "        E_to_out = E[..., :self.out_dim_E]\n",
    "        y_to_out = y[..., :self.out_dim_y]\n",
    "\n",
    "        new_E = self.mlp_in_E(E)\n",
    "        new_E = (new_E + new_E.transpose(1, 2)) / 2\n",
    "        logging.debug(f\"X shape: {X.shape}\")\n",
    "        after_in = utils.PlaceHolder(X=self.mlp_in_X(X), E=new_E, y=self.mlp_in_y(y)).mask(node_mask)\n",
    "        logging.debug(f\"after_in.X shape: {after_in.X.shape}\")\n",
    "        X, E, y = after_in.X, after_in.E, after_in.y\n",
    "\n",
    "        for layer in self.tf_layers:\n",
    "            X, E, y = layer(X, E, y, node_mask)\n",
    "\n",
    "        X = self.mlp_out_X(X)\n",
    "        E = self.mlp_out_E(E)\n",
    "        y = self.mlp_out_y(y)\n",
    "\n",
    "        X = (X + X_to_out)\n",
    "        E = (E + E_to_out) * diag_mask\n",
    "        y = y + y_to_out\n",
    "\n",
    "        E = 1/2 * (E + torch.transpose(E, 1, 2))\n",
    "\n",
    "        return utils.PlaceHolder(X=X, E=E, y=y).mask(node_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
