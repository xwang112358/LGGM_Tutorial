{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Introduction to Graph Diffusion Model in the Discrete Space\n",
    "\n",
    "\n",
    "Diffusion Model \n",
    "\n",
    "Graph Diffusion Model \n",
    "\n",
    "The **aims** of this tutorial are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch, remove_self_loops, to_undirected\n",
    "from torchmetrics import Metric, MeanSquaredError\n",
    "from torch import Tensor\n",
    "import math\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import os\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torchmetrics import Metric, MeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "In the first section of the tutorial, we only focus on the generation of graph topology which is the main novelty of the graph diffusion model. Later, we will apply the graph diffusion model on the molecular graph characterized with different atom and edge types, displaying the most popular application of graph generation model.\n",
    "\n",
    "### Examining the Raw Graph Topology\n",
    "\n",
    "The adjacency matrix $A \\in \\mathbb R^{n\\times n}$ where $n$ is the number of nodes in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]]) \n",
      " The shape of graph: torch.Size([150, 150])\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'datasets/raw_graph_data'\n",
    "dataset = torch.load(os.path.join(dataset_path, 'data.pt'))\n",
    "graph = dataset[0]\n",
    "print(graph, '\\n The shape of graph:', graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,  ..., 149, 149, 149],\n",
       "        [  0, 143, 145,  ..., 130, 136, 149]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nonzero().t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Objects and Data Loader with Pytorch Geometric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_object(data, mol=False):\n",
    "    if isinstance(data, torch.Tensor) and not mol: \n",
    "        n_nodes = data.shape[0] \n",
    "        edge_index = data.nonzero().t() \n",
    "        # each edge has uniform feature [0,1] as one-hot encoding\n",
    "        edge_attr = torch.tensor([[0, 1] for _ in range(edge_index.shape[1])]) \n",
    "\n",
    "        edge_index, edge_attr = to_undirected(edge_index, edge_attr, n_nodes, reduce = 'mean')\n",
    "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "\n",
    "        # each node has uniform feature 1 \n",
    "        x = torch.ones((n_nodes, 1))\n",
    "        # empty global feature\n",
    "        y = torch.empty(1, 0)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Data type not supported') \n",
    "\n",
    "def load_dataset(dataname, batch_size=1):\n",
    "    data_path = os.path.join('datasets', dataname)\n",
    "    if not os.path.exists(os.path.join('datasets',dataname, 'train.pt')):\n",
    "        raise ValueError('Dataset not found. Please run split_dataset first.')\n",
    "    else:\n",
    "        train_data = [create_data_object(_) for _ in torch.load(os.path.join(data_path, 'train.pt'))]\n",
    "        val_data = [create_data_object(_) for _ in torch.load(os.path.join(data_path, 'val.pt'))]\n",
    "        test_data = [create_data_object(_) for _ in torch.load(os.path.join(data_path, 'test.pt'))]\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)  \n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def split_dataset(dataname):\n",
    "    if not os.path.exists(os.path.join('datasets',dataname, 'train.pt')):\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        n = len(dataset)\n",
    "        indices = torch.randperm(n)\n",
    "        # 80% train, 10% validation, 10% test\n",
    "        train_indices = indices[:int(0.8 * n)]\n",
    "        val_indices = indices[int(0.8 * n):int(0.9 * n)]\n",
    "        test_indices = indices[int(0.9 * n):]\n",
    "\n",
    "        train_data = [dataset[_] for _ in train_indices]\n",
    "        val_data = [dataset[_] for _ in val_indices]\n",
    "        test_data = [dataset[_] for _ in test_indices]\n",
    "\n",
    "        # save the train, valid, test, indices\n",
    "        torch.save(train_indices, os.path.join(dataset_path, 'train_indices.pt'))\n",
    "        torch.save(val_indices, os.path.join(dataset_path, 'val_indices.pt'))  \n",
    "        torch.save(test_indices, os.path.join(dataset_path, 'test_indices.pt'))\n",
    "\n",
    "        # save the train, valid, test data\n",
    "        torch.save(train_data, os.path.join(dataset_path, 'train.pt'))\n",
    "        torch.save(val_data, os.path.join(dataset_path, 'val.pt'))\n",
    "        torch.save(test_data, os.path.join(dataset_path, 'test.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataset(dataname, batch_size):\n",
    "    train_loader, val_loader, test_loader = load_dataset(dataname, batch_size)\n",
    "\n",
    "    n_nodes = node_counts(1000, train_loader, val_loader)\n",
    "    node_types = torch.tensor([1]) \n",
    "    edge_types = edge_counts(train_loader)\n",
    "    \n",
    "    num_classes = len(node_types)\n",
    "    max_n_nodes = len(n_nodes) - 1\n",
    "    nodes_dist = DistributionNodes(n_nodes)\n",
    "    \n",
    "    data_loaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
    "\n",
    "    return data_loaders, num_classes, max_n_nodes, nodes_dist, edge_types, node_types, n_nodes\n",
    "\n",
    "\n",
    "def node_counts(max_nodes_possible, train_loader, val_loader):\n",
    "    #Count the distribution of graph size\n",
    "    all_counts = torch.zeros(max_nodes_possible)\n",
    "    \n",
    "    for loader in [train_loader, val_loader]:\n",
    "        for data in loader:\n",
    "            unique, counts = torch.unique(data.batch, return_counts=True)\n",
    "            for count in counts:\n",
    "                all_counts[count] += 1\n",
    "\n",
    "    max_index = max(all_counts.nonzero())\n",
    "    all_counts = all_counts[:max_index + 1]\n",
    "    all_counts = all_counts / all_counts.sum()\n",
    "    \n",
    "    return all_counts\n",
    "\n",
    "def node_types(train_loader):\n",
    "    #Count the marginal distribution of node types\n",
    "    num_classes = None\n",
    "    for data in train_loader:\n",
    "        num_classes = data.x.shape[1]\n",
    "        break\n",
    "\n",
    "    counts = torch.zeros(num_classes)\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        counts += data.x.sum(dim=0)\n",
    "\n",
    "    counts = counts / counts.sum()\n",
    "    return counts \n",
    "\n",
    "def edge_counts(train_loader):\n",
    "    #Count the marginal distribution of edge types\n",
    "    num_classes = None\n",
    "    for data in train_loader:\n",
    "        num_classes = data.edge_attr.shape[1]\n",
    "        break\n",
    "\n",
    "    d = torch.zeros(num_classes, dtype=torch.float)\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        unique, counts = torch.unique(data.batch, return_counts=True)\n",
    "\n",
    "        all_pairs = 0\n",
    "        for count in counts:\n",
    "            all_pairs += count * (count - 1)\n",
    "\n",
    "        \n",
    "        num_edges = data.edge_index.shape[1]\n",
    "        num_non_edges = all_pairs - num_edges\n",
    "    \n",
    "        edge_types = data.edge_attr.sum(dim=0)\n",
    "        assert num_non_edges >= 0\n",
    "        d[0] += num_non_edges\n",
    "        d[1:] += edge_types[1:]\n",
    "\n",
    "    d = d / d.sum()\n",
    "    return d\n",
    "\n",
    "\n",
    "class DistributionNodes:\n",
    "    def __init__(self, histogram):\n",
    "        \"\"\" Compute the distribution of the number of nodes in the dataset, and sample from this distribution.\n",
    "            historgram: dict. The keys are num_nodes, the values are counts\n",
    "        \"\"\"\n",
    "\n",
    "        if type(histogram) == dict:\n",
    "            max_n_nodes = max(histogram.keys())\n",
    "            prob = torch.zeros(max_n_nodes + 1)\n",
    "            for num_nodes, count in histogram.items():\n",
    "                prob[num_nodes] = count\n",
    "        else:\n",
    "            prob = histogram\n",
    "\n",
    "        self.prob = prob / prob.sum()\n",
    "        self.m = torch.distributions.Categorical(prob)\n",
    "\n",
    "    def sample_n(self, n_samples, device):\n",
    "        idx = self.m.sample((n_samples,))\n",
    "        return idx.to(device)\n",
    "\n",
    "    def log_prob(self, batch_n_nodes):\n",
    "        assert len(batch_n_nodes.size()) == 1\n",
    "        p = self.prob.to(batch_n_nodes.device)\n",
    "\n",
    "        mask = batch_n_nodes >= p.shape[0]\n",
    "        batch_n_nodes[mask] = p.shape[0] - 1\n",
    "\n",
    "        probas = p[batch_n_nodes]\n",
    "\n",
    "        probas[mask] = 0\n",
    "        log_p = torch.log(probas + 1e-30)\n",
    "        \n",
    "        return log_p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important data helpers\n",
    "def encode_no_edge(E): # E: bs, n, n, edge_attr\n",
    "    assert len(E.shape) == 4\n",
    "    if E.shape[-1] == 0:\n",
    "        return E\n",
    "    # edge [0,1] no edge [0,0]\n",
    "    no_edge = torch.sum(E, dim=3) == 0\n",
    "    first_elt = E[:, :, :, 0]\n",
    "    first_elt[no_edge] = 1\n",
    "    E[:, :, :, 0] = first_elt   \n",
    "    \n",
    "    diag = torch.eye(E.shape[1], dtype=torch.bool).unsqueeze(0).expand(E.shape[0], -1, -1)\n",
    "    E[diag] = 0 # prevent self loops\n",
    "    return E\n",
    "\n",
    "class PlaceHolder:\n",
    "    def __init__(self, X, E, y):\n",
    "        self.X = X\n",
    "        self.E = E\n",
    "        self.y = y\n",
    "\n",
    "    def type_as(self, x: torch.Tensor):\n",
    "        \"\"\" Changes the device and dtype of X, E, y. \"\"\"\n",
    "        self.X = self.X.type_as(x)\n",
    "        self.E = self.E.type_as(x)\n",
    "        self.y = self.y.type_as(x)\n",
    "        return self\n",
    "\n",
    "    def mask(self, node_mask, collapse=False):\n",
    "        x_mask = node_mask.unsqueeze(-1)          # bs, n, 1\n",
    "        e_mask1 = x_mask.unsqueeze(2)             # bs, n, 1, 1\n",
    "        e_mask2 = x_mask.unsqueeze(1)             # bs, 1, n, 1\n",
    "\n",
    "        if collapse:\n",
    "            self.X = torch.argmax(self.X, dim=-1)\n",
    "            self.E = torch.argmax(self.E, dim=-1)\n",
    "\n",
    "            self.X[node_mask == 0] = - 1\n",
    "            self.E[(e_mask1 * e_mask2).squeeze(-1) == 0] = - 1\n",
    "        else:\n",
    "            self.X = self.X * x_mask\n",
    "            self.E = self.E * e_mask1 * e_mask2\n",
    "            assert torch.allclose(self.E, torch.transpose(self.E, 1, 2))\n",
    "        return self\n",
    "\n",
    "def to_dense(x, edge_index, edge_attr, batch):\n",
    "    X, node_mask = to_dense_batch(x=x, batch=batch)\n",
    "    edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "    \n",
    "    max_num_nodes = X.size(1)\n",
    "    E = to_dense_adj(edge_index=edge_index, batch=batch, edge_attr=edge_attr, max_num_nodes=max_num_nodes)\n",
    "    E = encode_no_edge(E)\n",
    "\n",
    "    return PlaceHolder(X=X, E=E, y=None), node_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of node types: 1\n",
      "maximum number of nodes: 150\n",
      "Distributuon of Node Types: tensor([1])\n",
      "Distributuon of Edge Types: tensor([0.9730, 0.0270])\n",
      "Distributuon of Graph Sizes: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0051, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0102, 0.0000, 0.0000, 0.0051, 0.0051, 0.0000,\n",
      "        0.0051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0051, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0051,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0051, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9442])\n",
      "check original batch data\n",
      "X: torch.Size([577, 1])\n",
      "E: torch.Size([2, 2520])\n",
      "check dense batch data\n",
      "check edge_index ................ torch.float32\n",
      "X: torch.Size([4, 150, 1])\n",
      "E torch.Size([4, 150, 150, 2])\n"
     ]
    }
   ],
   "source": [
    "data_loaders, num_classes, max_n_nodes, nodes_dist, edge_types, node_types, n_nodes = init_dataset('raw_graph_data', batch_size=4)\n",
    "train_loader = data_loaders['train']\n",
    "\n",
    "print('number of node types:', num_classes) \n",
    "print('maximum number of nodes:', max_n_nodes)\n",
    "print('Distributuon of Node Types:', node_types)\n",
    "print('Distributuon of Edge Types:', edge_types) \n",
    "print('Distributuon of Graph Sizes:', n_nodes)\n",
    "\n",
    "print('check original batch data')\n",
    "for batch in train_loader:\n",
    "    print('X:', batch.x.shape)\n",
    "    print('E:', batch.edge_index.shape)\n",
    "    break\n",
    "\n",
    "print('check dense batch data')\n",
    "for batch in train_loader:\n",
    "    dense_data, node_mask = to_dense(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "    print('X:', dense_data.X.shape)\n",
    "    print('E', dense_data.E.shape)\n",
    "    break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Feature\n",
    "\n",
    "Calculate extra features from the noisy graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extra_features import NodeCycleFeatures, EigenFeatures\n",
    "\n",
    "class ExtraFeatures:\n",
    "    def __init__(self, extra_features_type, max_n_nodes):\n",
    "        self.max_n_nodes = max_n_nodes\n",
    "        self.ncycles = NodeCycleFeatures()\n",
    "        self.features_type = extra_features_type\n",
    "        if extra_features_type in ['eigenvalues', 'all']:\n",
    "            self.eigenfeatures = EigenFeatures(mode=extra_features_type)\n",
    "\n",
    "    def __call__(self, noisy_data):\n",
    "        n = noisy_data['node_mask'].sum(dim=1).unsqueeze(1) / self.max_n_nodes\n",
    "        x_cycles, y_cycles = self.ncycles(noisy_data)       # (bs, n_cycles)\n",
    "\n",
    "        if self.features_type == 'cycles':\n",
    "            E = noisy_data['E_t']\n",
    "            extra_edge_attr = torch.zeros((*E.shape[:-1], 0)).type_as(E)\n",
    "            return PlaceHolder(X=x_cycles, E=extra_edge_attr, y=torch.hstack((n, y_cycles)))\n",
    "\n",
    "        elif self.features_type == 'eigenvalues':\n",
    "            eigenfeatures = self.eigenfeatures(noisy_data)\n",
    "            E = noisy_data['E_t']\n",
    "            extra_edge_attr = torch.zeros((*E.shape[:-1], 0)).type_as(E)\n",
    "            n_components, batched_eigenvalues = eigenfeatures   # (bs, 1), (bs, 10)\n",
    "            return PlaceHolder(X=x_cycles, E=extra_edge_attr, y=torch.hstack((n, y_cycles, n_components,\n",
    "                                                                                    batched_eigenvalues)))\n",
    "        elif self.features_type == 'all':\n",
    "            eigenfeatures = self.eigenfeatures(noisy_data)\n",
    "            E = noisy_data['E_t']\n",
    "            extra_edge_attr = torch.zeros((*E.shape[:-1], 0)).type_as(E)\n",
    "            n_components, batched_eigenvalues, nonlcc_indicator, k_lowest_eigvec = eigenfeatures   # (bs, 1), (bs, 10),\n",
    "                                                                                                # (bs, n, 1), (bs, n, 2)\n",
    "\n",
    "            return PlaceHolder(X=torch.cat((x_cycles, nonlcc_indicator, k_lowest_eigvec), dim=-1),\n",
    "                                     E=extra_edge_attr,\n",
    "                                     y=torch.hstack((n, y_cycles, n_components, batched_eigenvalues)))\n",
    "        else:\n",
    "            raise ValueError(f\"Features type {self.features_type} not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ExtraFeatures object at 0x7ac1dcb2b280>\n"
     ]
    }
   ],
   "source": [
    "# calculate and print the extra features for the first batch\n",
    "extra_features = ExtraFeatures('all', max_n_nodes)\n",
    "print(extra_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the node and edge matrices in a batch to be consistent during the training and sampling, we create a **PlaceHolder** to store $X, E, y$ which is able to mask the actual node in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Scheduler (Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteUniformTransition:\n",
    "    def __init__(self, x_classes: int, e_classes: int, y_classes: int):\n",
    "        self.X_classes = x_classes\n",
    "        self.E_classes = e_classes\n",
    "        self.y_classes = y_classes\n",
    "        self.u_x = torch.ones(1, self.X_classes, self.X_classes)\n",
    "        if self.X_classes > 0:\n",
    "            self.u_x = self.u_x / self.X_classes\n",
    "\n",
    "        self.u_e = torch.ones(1, self.E_classes, self.E_classes)\n",
    "        if self.E_classes > 0:\n",
    "            self.u_e = self.u_e / self.E_classes\n",
    "\n",
    "        self.u_y = torch.ones(1, self.y_classes, self.y_classes)\n",
    "        if self.y_classes > 0:\n",
    "            self.u_y = self.u_y / self.y_classes\n",
    "\n",
    "    def get_Qt(self, beta_t, device):\n",
    "        \"\"\" Returns one-step transition matrices for X and E, from step t - 1 to step t.\n",
    "        Qt = (1 - beta_t) * I + beta_t / K\n",
    "\n",
    "        beta_t: (bs)                         noise level between 0 and 1\n",
    "        returns: qx (bs, dx, dx), qe (bs, de, de), qy (bs, dy, dy).\n",
    "        \"\"\"\n",
    "        beta_t = beta_t.unsqueeze(1)\n",
    "        beta_t = beta_t.to(device)\n",
    "        self.u_x = self.u_x.to(device)\n",
    "        self.u_e = self.u_e.to(device)\n",
    "        self.u_y = self.u_y.to(device)\n",
    "\n",
    "        q_x = beta_t * self.u_x + (1 - beta_t) * torch.eye(self.X_classes, device=device).unsqueeze(0)\n",
    "        q_e = beta_t * self.u_e + (1 - beta_t) * torch.eye(self.E_classes, device=device).unsqueeze(0)\n",
    "        q_y = beta_t * self.u_y + (1 - beta_t) * torch.eye(self.y_classes, device=device).unsqueeze(0)\n",
    "\n",
    "        return PlaceHolder(X=q_x, E=q_e, y=q_y)\n",
    "\n",
    "    def get_Qt_bar(self, alpha_bar_t, device):\n",
    "        \"\"\" Returns t-step transition matrices for X and E, from step 0 to step t.\n",
    "        Qt = prod(1 - beta_t) * I + (1 - prod(1 - beta_t)) / K\n",
    "\n",
    "        alpha_bar_t: (bs)         Product of the (1 - beta_t) for each time step from 0 to t.\n",
    "        returns: qx (bs, dx, dx), qe (bs, de, de), qy (bs, dy, dy).\n",
    "        \"\"\"\n",
    "        alpha_bar_t = alpha_bar_t.unsqueeze(1)\n",
    "        alpha_bar_t = alpha_bar_t.to(device)\n",
    "        self.u_x = self.u_x.to(device)\n",
    "        self.u_e = self.u_e.to(device)\n",
    "        self.u_y = self.u_y.to(device)\n",
    "\n",
    "        q_x = alpha_bar_t * torch.eye(self.X_classes, device=device).unsqueeze(0) + (1 - alpha_bar_t) * self.u_x\n",
    "        q_e = alpha_bar_t * torch.eye(self.E_classes, device=device).unsqueeze(0) + (1 - alpha_bar_t) * self.u_e\n",
    "        q_y = alpha_bar_t * torch.eye(self.y_classes, device=device).unsqueeze(0) + (1 - alpha_bar_t) * self.u_y\n",
    "\n",
    "        return PlaceHolder(X=q_x, E=q_e, y=q_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule_discrete(timesteps, s=0.008):\n",
    "    \"\"\" Cosine schedule as proposed in https://openreview.net/forum?id=-NEXDKk8gZ. \"\"\"\n",
    "    steps = timesteps + 2\n",
    "    x = np.linspace(0, steps, steps)\n",
    "\n",
    "    alphas_cumprod = np.cos(0.5 * np.pi * ((x / steps) + s) / (1 + s)) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    alphas = (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    betas = 1 - alphas\n",
    "    return betas.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredefinedNoiseScheduleDiscrete(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Predefined noise schedule. Essentially creates a lookup array for predefined (non-learned) noise schedules.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise_schedule, timesteps):\n",
    "        super(PredefinedNoiseScheduleDiscrete, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        if noise_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule_discrete(timesteps)\n",
    "        else:\n",
    "            raise NotImplementedError(noise_schedule)\n",
    "\n",
    "        self.register_buffer('betas', torch.from_numpy(betas).float())\n",
    "        self.alphas = 1 - torch.clamp(self.betas, min=0, max=0.9999)\n",
    "\n",
    "        log_alpha = torch.log(self.alphas)\n",
    "        log_alpha_bar = torch.cumsum(log_alpha, dim=0)\n",
    "        self.alphas_bar = torch.exp(log_alpha_bar)\n",
    "        # print(f\"[Noise schedule: {noise_schedule}] alpha_bar:\", self.alphas_bar)\n",
    "\n",
    "    def forward(self, t_normalized=None, t_int=None):\n",
    "        assert int(t_normalized is None) + int(t_int is None) == 1\n",
    "        if t_int is None:\n",
    "            t_int = torch.round(t_normalized * self.timesteps)\n",
    "        return self.betas[t_int.long()]\n",
    "\n",
    "    def get_alpha_bar(self, t_normalized=None, t_int=None):\n",
    "        assert int(t_normalized is None) + int(t_int is None) == 1\n",
    "        if t_int is None:\n",
    "            t_int = torch.round(t_normalized * self.timesteps)\n",
    "        return self.alphas_bar.to(t_int.device)[t_int.long()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_noise(self, X, E, y, node_mask, augment = False):\n",
    "    \"\"\" Sample noise and apply it to the data. \"\"\"\n",
    "\n",
    "    # Sample a timestep t.\n",
    "    # When evaluating, the loss for t=0 is computed separately\n",
    "    if not augment:\n",
    "        lowest_t = 0 if self.training else 1\n",
    "        t_int = torch.randint(lowest_t, self.T + 1, size=(X.size(0), 1), device=X.device).float()  # (bs, 1)\n",
    "        s_int = t_int - 1\n",
    "\n",
    "        t_float = t_int / self.T\n",
    "        s_float = s_int / self.T\n",
    "    else:\n",
    "        t_int = self.aug_steps * torch.ones(X.size(0), 1, device = X.device).float()\n",
    "        s_int = t_int - 1\n",
    "\n",
    "        t_float = t_int / self.aug_steps\n",
    "        s_float = s_int / self.aug_steps\n",
    "\n",
    "    # beta_t and alpha_s_bar are used for denoising/loss computation\n",
    "    beta_t = self.noise_schedule(t_normalized=t_float)                         # (bs, 1)\n",
    "    alpha_s_bar = self.noise_schedule.get_alpha_bar(t_normalized=s_float)      # (bs, 1)\n",
    "    alpha_t_bar = self.noise_schedule.get_alpha_bar(t_normalized=t_float)      # (bs, 1)\n",
    "\n",
    "    Qtb = self.transition_model.get_Qt_bar(alpha_t_bar, device=self.device)  # (bs, dx_in, dx_out), (bs, de_in, de_out)\n",
    "    assert (abs(Qtb.X.sum(dim=2) - 1.) < 1e-4).all(), Qtb.X.sum(dim=2) - 1\n",
    "    assert (abs(Qtb.E.sum(dim=2) - 1.) < 1e-4).all()\n",
    "\n",
    "    # Compute transition probabilities\n",
    "    probX = X @ Qtb.X  # (bs, n, dx_out)\n",
    "    probE = E @ Qtb.E.unsqueeze(1)  # (bs, n, n, de_out)\n",
    "\n",
    "    sampled_t = diffusion_utils.sample_discrete_features(probX=probX, probE=probE, node_mask=node_mask)\n",
    "\n",
    "    X_t = F.one_hot(sampled_t.X, num_classes=self.Xdim_output)\n",
    "    E_t = F.one_hot(sampled_t.E, num_classes=self.Edim_output)\n",
    "    assert (X.shape == X_t.shape) and (E.shape == E_t.shape)\n",
    "\n",
    "    z_t = PlaceHolder(X=X_t, E=E_t, y=y).type_as(X_t).mask(node_mask)\n",
    "\n",
    "    noisy_data = {'t_int': t_int, 't': t_float, 'beta_t': beta_t, 'alpha_s_bar': alpha_s_bar,\n",
    "                    'alpha_t_bar': alpha_t_bar, 'X_t': z_t.X, 'E_t': z_t.E, 'y_t': z_t.y, 'node_mask': node_mask}\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "We utilize [TORCHMETRICS](https://lightning.ai/docs/torchmetrics/stable/references/metric.html) to implement our train loss, validation and test metrics. The class can help us \n",
    "\n",
    "1. Handles the transfer of metric states to correct device \n",
    "2. Handles the synchronization of metric states across processes \n",
    "\n",
    "The three core methods of the base class are `add_state()`, `forward()`, and `reset()` which should never be overwritten by child classes. Instead, we should overwrite `update()` and `compute()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_ce', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: Tensor, target: Tensor) -> None:\n",
    "        \"\"\" Update state with predictions and targets.\n",
    "            preds: Predictions from model   (bs * n, d) or (bs * n * n, d)\n",
    "            target: Ground truth values     (bs * n, d) or (bs * n * n, d). \"\"\"\n",
    "        target = torch.argmax(target, dim=-1)\n",
    "        output = F.cross_entropy(preds, target, reduction='sum')\n",
    "        self.total_ce += output\n",
    "        self.total_samples += preds.size(0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_ce / self.total_samples\n",
    "\n",
    "\n",
    "class TrainLossDiscrete(nn.Module):\n",
    "    \"\"\" Train with Cross entropy\"\"\"\n",
    "    def __init__(self, lambda_train):\n",
    "        super().__init__()\n",
    "        self.node_loss = CrossEntropyMetric()\n",
    "        self.edge_loss = CrossEntropyMetric()\n",
    "        self.y_loss = CrossEntropyMetric()\n",
    "        self.lambda_train = lambda_train\n",
    "\n",
    "    def forward(self, masked_pred_X, masked_pred_E, pred_y, true_X, true_E, true_y, log: bool):\n",
    "        \"\"\" Compute train metrics\n",
    "        masked_pred_X : tensor -- (bs, n, dx)\n",
    "        masked_pred_E : tensor -- (bs, n, n, de)\n",
    "        pred_y : tensor -- (bs, )\n",
    "        true_X : tensor -- (bs, n, dx)\n",
    "        true_E : tensor -- (bs, n, n, de)\n",
    "        true_y : tensor -- (bs, )\n",
    "        log : boolean. \"\"\"\n",
    "        true_X = torch.reshape(true_X, (-1, true_X.size(-1)))  # (bs * n, dx)\n",
    "        true_E = torch.reshape(true_E, (-1, true_E.size(-1)))  # (bs * n * n, de)\n",
    "        masked_pred_X = torch.reshape(masked_pred_X, (-1, masked_pred_X.size(-1)))  # (bs * n, dx)\n",
    "        masked_pred_E = torch.reshape(masked_pred_E, (-1, masked_pred_E.size(-1)))   # (bs * n * n, de)\n",
    "\n",
    "        # Remove masked rows\n",
    "        mask_X = (true_X != 0.).any(dim=-1)\n",
    "        mask_E = (true_E != 0.).any(dim=-1)\n",
    "\n",
    "        flat_true_X = true_X[mask_X, :]\n",
    "        flat_pred_X = masked_pred_X[mask_X, :]\n",
    "\n",
    "        flat_true_E = true_E[mask_E, :]\n",
    "        flat_pred_E = masked_pred_E[mask_E, :]\n",
    "\n",
    "        loss_X = self.node_loss(flat_pred_X, flat_true_X) if true_X.numel() > 0 else 0.0\n",
    "        loss_E = self.edge_loss(flat_pred_E, flat_true_E) if true_E.numel() > 0 else 0.0\n",
    "        loss_y = self.y_loss(pred_y, true_y) if true_y.numel() > 0 else 0.0\n",
    "        \n",
    "        return loss_X + self.lambda_train[0] * loss_E + self.lambda_train[1] * loss_y\n",
    "\n",
    "    def reset(self):\n",
    "        for metric in [self.node_loss, self.edge_loss, self.y_loss]:\n",
    "            metric.reset()\n",
    "\n",
    "    def log_epoch_metrics(self):\n",
    "        epoch_node_loss = self.node_loss.compute() if self.node_loss.total_samples > 0 else -1\n",
    "        epoch_edge_loss = self.edge_loss.compute() if self.edge_loss.total_samples > 0 else -1\n",
    "        epoch_y_loss = self.train_y_loss.compute() if self.y_loss.total_samples > 0 else -1\n",
    "\n",
    "        to_log = {\"train_epoch/x_CE\": epoch_node_loss,\n",
    "                  \"train_epoch/E_CE\": epoch_edge_loss,\n",
    "                  \"train_epoch/y_CE\": epoch_y_loss}\n",
    "\n",
    "        return to_log\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation & Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_nll', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, batch_nll) -> None:\n",
    "        self.total_nll += torch.sum(batch_nll)\n",
    "        self.total_samples += batch_nll.numel()\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_nll / self.total_samples\n",
    "    \n",
    "class SumExceptBatchKL(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_value', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, p, q) -> None:\n",
    "        self.total_value += F.kl_div(q, p, reduction='sum')\n",
    "        self.total_samples += p.size(0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_value / self.total_samples\n",
    "    \n",
    "class SumExceptBatchMetric(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state('total_value', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "        self.add_state('total_samples', default=torch.tensor(0.), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, values) -> None:\n",
    "        self.total_value += torch.sum(values)\n",
    "        self.total_samples += values.shape[0]\n",
    "\n",
    "    def compute(self):\n",
    "        return self.total_value / self.total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import concurrent\n",
    "from src.analysis.dist_helper import disc, gaussian_emd, gaussian_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_mmd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
    "    ''' MMD between two samples '''\n",
    "    # normalize histograms into pmf\n",
    "    if is_hist:\n",
    "        samples1 = [s1 / (np.sum(s1) + 1e-6) for s1 in samples1]\n",
    "        samples2 = [s2 / (np.sum(s2) + 1e-6) for s2 in samples2]\n",
    "    return disc(samples1, samples1, kernel, *args, **kwargs) + disc(samples2, samples2, kernel, *args, **kwargs) - \\\n",
    "                2 * disc(samples1, samples2, kernel, *args, **kwargs)\n",
    "\n",
    "\n",
    "def compute_emd(samples1, samples2, kernel, is_hist=True, *args, **kwargs):\n",
    "    ''' EMD between average of two samples '''\n",
    "    # normalize histograms into pmf\n",
    "    if is_hist:\n",
    "        samples1 = [np.mean(samples1)]\n",
    "        samples2 = [np.mean(samples2)]\n",
    "    return disc(samples1, samples2, kernel, *args,\n",
    "                            **kwargs), [samples1[0], samples2[0]]\n",
    "\n",
    "\n",
    "def degree_worker(G):\n",
    "    return np.array(nx.degree_histogram(G))\n",
    "\n",
    "def degree_stats(graph_ref_list, graph_pred_list, is_parallel=True, compute_emd=False):\n",
    "    ''' Compute the distance between the degree distributions of two unordered sets of graphs.\n",
    "        Args:\n",
    "            graph_ref_list, graph_target_list: two lists of networkx graphs to be evaluated\n",
    "        '''\n",
    "    sample_ref = []\n",
    "    sample_pred = []\n",
    "    # in case an empty graph is generated\n",
    "    graph_pred_list_remove_empty = [\n",
    "        G for G in graph_pred_list if not G.number_of_nodes() == 0\n",
    "    ]\n",
    "\n",
    "    prev = datetime.now()\n",
    "    if is_parallel:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for deg_hist in executor.map(degree_worker, graph_ref_list):\n",
    "                sample_ref.append(deg_hist)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for deg_hist in executor.map(degree_worker, graph_pred_list_remove_empty):\n",
    "                sample_pred.append(deg_hist)\n",
    "    else:\n",
    "        for i in range(len(graph_ref_list)):\n",
    "            degree_temp = np.array(nx.degree_histogram(graph_ref_list[i]))\n",
    "            sample_ref.append(degree_temp)\n",
    "        for i in range(len(graph_pred_list_remove_empty)):\n",
    "            degree_temp = np.array(\n",
    "                nx.degree_histogram(graph_pred_list_remove_empty[i]))\n",
    "            sample_pred.append(degree_temp)\n",
    "\n",
    "    if compute_emd:\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_emd)\n",
    "    else:\n",
    "        mmd_dist = compute_mmd(sample_ref, sample_pred, kernel=gaussian_tv)\n",
    "\n",
    "    elapsed = datetime.now() - prev\n",
    "\n",
    "    print('Time computing degree mmd: ', elapsed)\n",
    "    return mmd_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectreSamplingMetrics(nn.Module):\n",
    "    def __init__(self, data_loaders, compute_emd, metrics_list):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_graphs = self.loader_to_nx(data_loaders['train'])\n",
    "        self.val_graphs = self.loader_to_nx(data_loaders['val'])\n",
    "        self.test_graphs = self.loader_to_nx(data_loaders['test'])\n",
    "        self.num_graphs_test = len(self.test_graphs)\n",
    "        self.num_graphs_val = len(self.val_graphs)\n",
    "        self.compute_emd = compute_emd\n",
    "        self.metrics_list = metrics_list\n",
    "\n",
    "    def loader_to_nx(self, loader):\n",
    "        networkx_graphs = []\n",
    "        for i, batch in enumerate(loader):\n",
    "            data_list = batch.to_data_list()\n",
    "            for j, data in enumerate(data_list):\n",
    "                networkx_graphs.append(to_networkx(data, node_attrs=None, edge_attrs=None, to_undirected=True,\n",
    "                                                   remove_self_loops=True))\n",
    "        return networkx_graphs\n",
    "\n",
    "    def forward(self, generated_graphs: list, local_rank, test=False):\n",
    "        reference_graphs = self.test_graphs if test else self.val_graphs\n",
    "        if local_rank == 0:\n",
    "            print(f\"Computing sampling metrics between {len(generated_graphs)} generated graphs and {len(reference_graphs)}\"\n",
    "                  f\" test graphs -- emd computation: {self.compute_emd}\")\n",
    "        networkx_graphs = []\n",
    "        adjacency_matrices = []\n",
    "        if local_rank == 0:\n",
    "            print(\"Building networkx graphs...\")\n",
    "        for graph in generated_graphs:\n",
    "            node_types, edge_types = graph\n",
    "            A = edge_types.bool().cpu().numpy()\n",
    "            adjacency_matrices.append(A)\n",
    "\n",
    "            nx_graph = nx.from_numpy_array(A)\n",
    "            networkx_graphs.append(nx_graph)\n",
    "\n",
    "        np.savez('generated_adjs.npz', *adjacency_matrices)\n",
    "\n",
    "        to_log = {}\n",
    "        if 'degree' in self.metrics_list:\n",
    "            if local_rank == 0:\n",
    "                print(\"Computing degree stats..\")\n",
    "            degree = degree_stats(reference_graphs, networkx_graphs, is_parallel=True,\n",
    "                                  compute_emd=self.compute_emd)\n",
    "            \n",
    "            to_log['degree'] = degree\n",
    "\n",
    "        if 'spectre' in self.metrics_list:\n",
    "            if local_rank == 0:\n",
    "                print(\"Computing spectre stats...\")\n",
    "            spectre = spectral_stats(reference_graphs, networkx_graphs, is_parallel=True, n_eigvals=-1,\n",
    "                                     compute_emd=self.compute_emd)\n",
    "\n",
    "            to_log['spectre'] = spectre\n",
    "\n",
    "        if 'clustering' in self.metrics_list:\n",
    "            if local_rank == 0:\n",
    "                print(\"Computing clustering stats...\")\n",
    "            clustering = clustering_stats(reference_graphs, networkx_graphs, bins=100, is_parallel=True,\n",
    "                                          compute_emd=self.compute_emd)\n",
    "            to_log['clustering'] = clustering\n",
    "\n",
    "        if 'orbit' in self.metrics_list:\n",
    "            if local_rank == 0:\n",
    "                print(\"Computing orbit stats...\")\n",
    "            orbit = orbit_stats_all(reference_graphs, networkx_graphs, compute_emd=self.compute_emd)\n",
    "            to_log['orbit'] = orbit\n",
    "\n",
    "        if local_rank == 0:\n",
    "            print(\"Sampling statistics\", to_log)\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CrossDomainSamplingMetrics(SpectreSamplingMetrics):\n",
    "    def __init__(self, data_loaders):\n",
    "        super().__init__(data_loaders=data_loaders,\n",
    "                         compute_emd=False,\n",
    "                         metrics_list=['degree', 'clustering', 'orbit', 'spectre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "We utilize [TORCHMETRICS](https://lightning.ai/docs/torchmetrics/stable/references/metric.html) to implement our train loss, validation and test metrics. The class can help us \n",
    "\n",
    "1. Handles the transfer of metric states to correct device \n",
    "2. Handles the synchronization of metric states across processes \n",
    "\n",
    "The three core methods of the base class are `add_state()`, `forward()`, and `reset()` which should never be overwritten by child classes. Instead, we should overwrite `update()` and `compute()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Transformer $\\phi_{\\theta}$: the Denoising Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xtoy(nn.Module):\n",
    "    def __init__(self, dx, dy):\n",
    "        \"\"\" Map node features to global features \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * dx, dy)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" X: bs, n, dx. \"\"\"\n",
    "        m = X.mean(dim=1)\n",
    "        mi = X.min(dim=1)[0]\n",
    "        ma = X.max(dim=1)[0]\n",
    "        std = X.std(dim=1)\n",
    "        z = torch.hstack((m, mi, ma, std))\n",
    "        out = self.lin(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Etoy(nn.Module):\n",
    "    def __init__(self, d, dy):\n",
    "        \"\"\" Map edge features to global features. \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4 * d, dy)\n",
    "\n",
    "    def forward(self, E):\n",
    "        \"\"\" E: bs, n, n, de\n",
    "            Features relative to the diagonal of E could potentially be added.\n",
    "        \"\"\"\n",
    "        m = E.mean(dim=(1, 2))\n",
    "        mi = E.min(dim=2)[0].min(dim=1)[0]\n",
    "        ma = E.max(dim=2)[0].max(dim=1)[0]\n",
    "        std = torch.std(E, dim=(1, 2))\n",
    "        z = torch.hstack((m, mi, ma, std))\n",
    "        out = self.lin(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "def masked_softmax(x, mask, **kwargs):\n",
    "    if mask.sum() == 0:\n",
    "        return x\n",
    "    x_masked = x.clone()\n",
    "    x_masked[mask == 0] = -float(\"inf\")\n",
    "    return torch.softmax(x_masked, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XEyTransformerLayer(nn.Module):\n",
    "    \"\"\" Transformer that updates node, edge and global features\n",
    "        d_x: node features\n",
    "        d_e: edge features\n",
    "        dz : global features\n",
    "        n_head: the number of heads in the multi_head_attention\n",
    "        dim_feedforward: the dimension of the feedforward network model after self-attention\n",
    "        dropout: dropout probablility. 0 to disable\n",
    "        layer_norm_eps: eps value in layer normalizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, dx: int, de: int, dy: int, n_head: int, dim_ffX: int = 2048,\n",
    "                 dim_ffE: int = 128, dim_ffy: int = 2048, dropout: float = 0.1,\n",
    "                 layer_norm_eps: float = 1e-5, device=None, dtype=None) -> None:\n",
    "        kw = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = NodeEdgeBlock(dx, de, dy, n_head, **kw)\n",
    "\n",
    "        self.linX1 = Linear(dx, dim_ffX, **kw)\n",
    "        self.linX2 = Linear(dim_ffX, dx, **kw)\n",
    "        self.normX1 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.normX2 = LayerNorm(dx, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutX1 = Dropout(dropout)\n",
    "        self.dropoutX2 = Dropout(dropout)\n",
    "        self.dropoutX3 = Dropout(dropout)\n",
    "\n",
    "        self.linE1 = Linear(de, dim_ffE, **kw)\n",
    "        self.linE2 = Linear(dim_ffE, de, **kw)\n",
    "        self.normE1 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.normE2 = LayerNorm(de, eps=layer_norm_eps, **kw)\n",
    "        self.dropoutE1 = Dropout(dropout)\n",
    "        self.dropoutE2 = Dropout(dropout)\n",
    "        self.dropoutE3 = Dropout(dropout)\n",
    "\n",
    "        self.lin_y1 = Linear(dy, dim_ffy, **kw)\n",
    "        self.lin_y2 = Linear(dim_ffy, dy, **kw)\n",
    "        self.norm_y1 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.norm_y2 = LayerNorm(dy, eps=layer_norm_eps, **kw)\n",
    "        self.dropout_y1 = Dropout(dropout)\n",
    "        self.dropout_y2 = Dropout(dropout)\n",
    "        self.dropout_y3 = Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def forward(self, X: Tensor, E: Tensor, y, node_mask: Tensor):\n",
    "        \"\"\" Pass the input through the encoder layer.\n",
    "            X: (bs, n, d)\n",
    "            E: (bs, n, n, d)\n",
    "            y: (bs, dy)\n",
    "            node_mask: (bs, n) Mask for the src keys per batch (optional)\n",
    "            Output: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "\n",
    "        newX, newE, new_y = self.self_attn(X, E, y, node_mask=node_mask)\n",
    "\n",
    "        newX_d = self.dropoutX1(newX)\n",
    "        X = self.normX1(X + newX_d)\n",
    "\n",
    "        newE_d = self.dropoutE1(newE)\n",
    "        E = self.normE1(E + newE_d)\n",
    "\n",
    "        new_y_d = self.dropout_y1(new_y)\n",
    "        y = self.norm_y1(y + new_y_d)\n",
    "\n",
    "        ff_outputX = self.linX2(self.dropoutX2(self.activation(self.linX1(X))))\n",
    "        ff_outputX = self.dropoutX3(ff_outputX)\n",
    "        X = self.normX2(X + ff_outputX)\n",
    "\n",
    "        ff_outputE = self.linE2(self.dropoutE2(self.activation(self.linE1(E))))\n",
    "        ff_outputE = self.dropoutE3(ff_outputE)\n",
    "        E = self.normE2(E + ff_outputE)\n",
    "\n",
    "        ff_output_y = self.lin_y2(self.dropout_y2(self.activation(self.lin_y1(y))))\n",
    "        ff_output_y = self.dropout_y3(ff_output_y)\n",
    "        y = self.norm_y2(y + ff_output_y)\n",
    "\n",
    "        return X, E, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEdgeBlock(nn.Module):\n",
    "    \"\"\" Self attention layer that also updates the representations on the edges. \"\"\"\n",
    "    def __init__(self, dx, de, dy, n_head, **kwargs):\n",
    "        super().__init__()\n",
    "        assert dx % n_head == 0, f\"dx: {dx} -- nhead: {n_head}\"\n",
    "        self.dx = dx\n",
    "        self.de = de\n",
    "        self.dy = dy\n",
    "        self.df = int(dx / n_head)\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # Attention\n",
    "        self.q = Linear(dx, dx)\n",
    "        self.k = Linear(dx, dx)\n",
    "        self.v = Linear(dx, dx)\n",
    "\n",
    "        # FiLM E to X\n",
    "        self.e_add = Linear(de, dx)\n",
    "        self.e_mul = Linear(de, dx)\n",
    "\n",
    "        # FiLM y to E\n",
    "        self.y_e_mul = Linear(dy, dx)           # Warning: here it's dx and not de\n",
    "        self.y_e_add = Linear(dy, dx)\n",
    "\n",
    "        # FiLM y to X\n",
    "        self.y_x_mul = Linear(dy, dx)\n",
    "        self.y_x_add = Linear(dy, dx)\n",
    "\n",
    "        # Process y\n",
    "        self.y_y = Linear(dy, dy)\n",
    "        self.x_y = Xtoy(dx, dy)\n",
    "        self.e_y = Etoy(de, dy)\n",
    "\n",
    "        # Output layers\n",
    "        self.x_out = Linear(dx, dx)\n",
    "        self.e_out = Linear(dx, de)\n",
    "        self.y_out = nn.Sequential(nn.Linear(dy, dy), nn.ReLU(), nn.Linear(dy, dy))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        \"\"\"\n",
    "        :param X: bs, n, d        node features\n",
    "        :param E: bs, n, n, d     edge features\n",
    "        :param y: bs, dz           global features\n",
    "        :param node_mask: bs, n\n",
    "        :return: newX, newE, new_y with the same shape.\n",
    "        \"\"\"\n",
    "        bs, n, _ = X.shape\n",
    "        x_mask = node_mask.unsqueeze(-1)        # bs, n, 1\n",
    "        e_mask1 = x_mask.unsqueeze(2)           # bs, n, 1, 1\n",
    "        e_mask2 = x_mask.unsqueeze(1)           # bs, 1, n, 1\n",
    "\n",
    "        # 1. Map X to keys and queries\n",
    "        Q = self.q(X) * x_mask           # (bs, n, dx)\n",
    "        K = self.k(X) * x_mask           # (bs, n, dx)\n",
    "        diffusion_utils.assert_correctly_masked(Q, x_mask)\n",
    "        # 2. Reshape to (bs, n, n_head, df) with dx = n_head * df\n",
    "\n",
    "        Q = Q.reshape((Q.size(0), Q.size(1), self.n_head, self.df))\n",
    "        K = K.reshape((K.size(0), K.size(1), self.n_head, self.df))\n",
    "\n",
    "        Q = Q.unsqueeze(2)                              # (bs, 1, n, n_head, df)\n",
    "        K = K.unsqueeze(1)                              # (bs, n, 1, n head, df)\n",
    "\n",
    "        # Compute unnormalized attentions. Y is (bs, n, n, n_head, df)\n",
    "        Y = Q * K\n",
    "        Y = Y / math.sqrt(Y.size(-1))\n",
    "        diffusion_utils.assert_correctly_masked(Y, (e_mask1 * e_mask2).unsqueeze(-1))\n",
    "\n",
    "        E1 = self.e_mul(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E1 = E1.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))\n",
    "\n",
    "        E2 = self.e_add(E) * e_mask1 * e_mask2                        # bs, n, n, dx\n",
    "        E2 = E2.reshape((E.size(0), E.size(1), E.size(2), self.n_head, self.df))\n",
    "\n",
    "        # Incorporate edge features to the self attention scores.\n",
    "        Y = Y * (E1 + 1) + E2                  # (bs, n, n, n_head, df)\n",
    "\n",
    "        # Incorporate y to E\n",
    "        newE = Y.flatten(start_dim=3)                      # bs, n, n, dx\n",
    "        ye1 = self.y_e_add(y).unsqueeze(1).unsqueeze(1)  # bs, 1, 1, de\n",
    "        ye2 = self.y_e_mul(y).unsqueeze(1).unsqueeze(1)\n",
    "        newE = ye1 + (ye2 + 1) * newE\n",
    "\n",
    "        # Output E\n",
    "        newE = self.e_out(newE) * e_mask1 * e_mask2      # bs, n, n, de\n",
    "        diffusion_utils.assert_correctly_masked(newE, e_mask1 * e_mask2)\n",
    "\n",
    "        # Compute attentions. attn is still (bs, n, n, n_head, df)\n",
    "        softmax_mask = e_mask2.expand(-1, n, -1, self.n_head)    # bs, 1, n, 1\n",
    "        attn = masked_softmax(Y, softmax_mask, dim=2)  # bs, n, n, n_head\n",
    "\n",
    "        V = self.v(X) * x_mask                        # bs, n, dx\n",
    "        V = V.reshape((V.size(0), V.size(1), self.n_head, self.df))\n",
    "        V = V.unsqueeze(1)                                     # (bs, 1, n, n_head, df)\n",
    "\n",
    "        # Compute weighted values\n",
    "        weighted_V = attn * V\n",
    "        weighted_V = weighted_V.sum(dim=2)\n",
    "\n",
    "        # Send output to input dim\n",
    "        weighted_V = weighted_V.flatten(start_dim=2)            # bs, n, dx\n",
    "\n",
    "        # Incorporate y to X\n",
    "        yx1 = self.y_x_add(y).unsqueeze(1)\n",
    "        yx2 = self.y_x_mul(y).unsqueeze(1)\n",
    "        newX = yx1 + (yx2 + 1) * weighted_V\n",
    "\n",
    "        # Output X\n",
    "        newX = self.x_out(newX) * x_mask\n",
    "        diffusion_utils.assert_correctly_masked(newX, x_mask)\n",
    "\n",
    "        # Process y based on X axnd E\n",
    "        y = self.y_y(y)\n",
    "        e_y = self.e_y(E)\n",
    "        x_y = self.x_y(X)\n",
    "        new_y = y + x_y + e_y\n",
    "        new_y = self.y_out(new_y)               # bs, dy\n",
    "\n",
    "        return newX, newE, new_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    n_layers : int -- number of layers\n",
    "    dims : dict -- contains dimensions for each feature type\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers: int, input_dims: dict, hidden_mlp_dims: dict, hidden_dims: dict,\n",
    "                 output_dims: dict, act_fn_in: nn.ReLU(), act_fn_out: nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.out_dim_X = output_dims['X']\n",
    "        self.out_dim_E = output_dims['E']\n",
    "        self.out_dim_y = output_dims['y']\n",
    "\n",
    "        self.mlp_in_X = nn.Sequential(nn.Linear(input_dims['X'], hidden_mlp_dims['X']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['X'], hidden_dims['dx']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_E = nn.Sequential(nn.Linear(input_dims['E'], hidden_mlp_dims['E']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['E'], hidden_dims['de']), act_fn_in)\n",
    "\n",
    "        self.mlp_in_y = nn.Sequential(nn.Linear(input_dims['y'], hidden_mlp_dims['y']), act_fn_in,\n",
    "                                      nn.Linear(hidden_mlp_dims['y'], hidden_dims['dy']), act_fn_in)\n",
    "\n",
    "        self.tf_layers = nn.ModuleList([XEyTransformerLayer(dx=hidden_dims['dx'],\n",
    "                                                            de=hidden_dims['de'],\n",
    "                                                            dy=hidden_dims['dy'],\n",
    "                                                            n_head=hidden_dims['n_head'],\n",
    "                                                            dim_ffX=hidden_dims['dim_ffX'],\n",
    "                                                            dim_ffE=hidden_dims['dim_ffE'])\n",
    "                                        for i in range(n_layers)])\n",
    "\n",
    "        self.mlp_out_X = nn.Sequential(nn.Linear(hidden_dims['dx'], hidden_mlp_dims['X']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['X'], output_dims['X']))\n",
    "\n",
    "        self.mlp_out_E = nn.Sequential(nn.Linear(hidden_dims['de'], hidden_mlp_dims['E']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['E'], output_dims['E']))\n",
    "\n",
    "        self.mlp_out_y = nn.Sequential(nn.Linear(hidden_dims['dy'], hidden_mlp_dims['y']), act_fn_out,\n",
    "                                       nn.Linear(hidden_mlp_dims['y'], output_dims['y']))\n",
    "\n",
    "    def forward(self, X, E, y, node_mask):\n",
    "        bs, n = X.shape[0], X.shape[1]\n",
    "\n",
    "        diag_mask = torch.eye(n)\n",
    "        diag_mask = ~diag_mask.type_as(E).bool()\n",
    "        diag_mask = diag_mask.unsqueeze(0).unsqueeze(-1).expand(bs, -1, -1, -1)\n",
    "\n",
    "        X_to_out = X[..., :self.out_dim_X]\n",
    "        E_to_out = E[..., :self.out_dim_E]\n",
    "        y_to_out = y[..., :self.out_dim_y]\n",
    "\n",
    "        new_E = self.mlp_in_E(E)\n",
    "        new_E = (new_E + new_E.transpose(1, 2)) / 2\n",
    "        logging.debug(f\"X shape: {X.shape}\")\n",
    "        after_in = utils.PlaceHolder(X=self.mlp_in_X(X), E=new_E, y=self.mlp_in_y(y)).mask(node_mask)\n",
    "        logging.debug(f\"after_in.X shape: {after_in.X.shape}\")\n",
    "        X, E, y = after_in.X, after_in.E, after_in.y\n",
    "\n",
    "        for layer in self.tf_layers:\n",
    "            X, E, y = layer(X, E, y, node_mask)\n",
    "\n",
    "        X = self.mlp_out_X(X)\n",
    "        E = self.mlp_out_E(E)\n",
    "        y = self.mlp_out_y(y)\n",
    "\n",
    "        X = (X + X_to_out)\n",
    "        E = (E + E_to_out) * diag_mask\n",
    "        y = y + y_to_out\n",
    "\n",
    "        E = 1/2 * (E + torch.transpose(E, 1, 2))\n",
    "\n",
    "        return utils.PlaceHolder(X=X, E=E, y=y).mask(node_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gad_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
